---
layout: wordpress-import
status: publish
published: true
title: 'Mashups of Bibliographic Data: A Report of the ALCTS Midwinter Forum'
modified: 2010-01-27T21:14:52+00:00
author: Peter Murray
author_login: lyrdor
author_email: jester@dltj.org
author_url: http://dltj.org/about
wordpress_id: 1478
wordpress_url: http://dltj.org/?p=1478
date: '2010-01-27 16:14:52 -0500'
date_gmt: '2010-01-27 21:14:52 -0500'
categories:
- Meeting
tags:
- MARC
- onix
- WorldCat
- OCLC
- Open Library
- Internet Archive
- Google Book Search
- ALA Midwinter Conference 2010
- Dewey Decimal Classification
- Association for Library Collections and Technical Services
comments:
- id: 42577
  author: Chris Blackall
  author_email: cblackall@gmail.com
  author_url: ''
  date: '2010-01-27 23:22:56 -0500'
  date_gmt: '2010-01-28 04:22:56 -0500'
  content: Thank you for the very informative and useful report.
- id: 42738
  author: Bryan Campbell
  author_email: campbellbryan2@gmail.com
  author_url: ''
  date: '2010-01-29 12:09:12 -0500'
  date_gmt: '2010-01-29 17:09:12 -0500'
  content: "On product dimensions. \r\n\r\nLibraries also pack and ship books (resource
    sharing/ILL) and store same locally on shelves and and in off-site storage facilities.
    \ I see this data being used to help libraries better manage their local collections
    (anticipating if there is enough shelving for incoming materials rather than after
    receipt, shifting, assigning locations in off-site storage ahead of time) and
    off-site storage facilities. Given these activities, you would think that libraries
    would be much more interested in product dimensions (besides height) and weight.
    \ \r\n\r\nBryan Campbell\r\nCharleston, SC"
- id: 42782
  author: 'HotStuff 2.0 &raquo; Blog Archive &raquo; Word of the Day: &#8220;instincts&#8221;'
  author_email: ''
  author_url: ''
  date: '2010-01-30 00:05:52 -0500'
  date_gmt: '2010-01-30 05:05:52 -0500'
  content: "[...] of Bibliographic Data: A Report of the ALCTS Midwinter Forum [web
    link]Disruptive Library Technology Jester (27/Jan/2010)&#8220;&#8230;libraries
    renee said our instincts [...]"
- id: 43230
  author: Peter Murray
  author_email: jester@dltj.org
  author_url: http://dltj.org/about
  date: '2010-02-02 19:35:06 -0500'
  date_gmt: '2010-02-03 00:35:06 -0500'
  content: That is a very good point, Bryan.  Something that probably wasn't considered
    when AACR2 was being drafted, but it is certainly more important in today's environment.  I
    wonder if this is something being considered by the NISO <a href="http://www.niso.org/workrooms/physdel"
    rel="nofollow">Physical Delivery of Library Resources Working Group</a>.
- id: 43469
  author: Le catalogue en d&eacute;bat (05/02/10) &laquo; pintiniblog
  author_email: ''
  author_url: http://pintiniblog.wordpress.com/2010/02/05/le-catalogue-en-debat-050210/
  date: '2010-02-05 02:12:51 -0500'
  date_gmt: '2010-02-05 07:12:51 -0500'
  content: "[...] Mashups of Bibliographic Data: A Report of the ALCTS Midwinter Forum
    (Disruptive Library Technology Jester, 27/01/2010) [...]"
- id: 43612
  author: 'Go To Hellman: Google Exposes Book Metadata Privates at ALA Forum'
  author_email: ''
  author_url: http://go-to-hellman.blogspot.com/2010/01/google-exposes-book-metadata-privates.html?showComment=1265282368735
  date: '2010-02-05 15:26:21 -0500'
  date_gmt: '2010-02-05 20:26:21 -0500'
  content: "<!--%kramer-ref-pre%-->[...] nice summary of the same session is over
    at Disruptive Library Technology [...]<!--%kramer-ref-post%-->"
- id: 43703
  author: 'The FRBR Blog: Blog Archive &raquo; Last Week in FRBR #14'
  author_email: ''
  author_url: http://www.frbr.org/2010/02/06/last-week-in-frbr-14
  date: '2010-02-06 15:34:32 -0500'
  date_gmt: '2010-02-06 20:34:32 -0500'
  content: "<!--%kramer-ref-pre%-->[...] Peter Murray was also there, and wrote it
    up in Mashups of Bibliographic Data: A Report of the ALCTS Midwinter Forum: [...]<!--%kramer-ref-post%-->"
- id: 44599
  author: Interesting Google Book Search Settlement Bits in Advance of Thursday&#8217;s
    Fairness Hearing | Disruptive Library Technology Jester
  author_email: ''
  author_url: http://dltj.org/article/interesting-gbs-bits/
  date: '2010-02-15 21:23:24 -0500'
  date_gmt: '2010-02-16 02:23:24 -0500'
  content: "[...] Thursday will be a big day in the Google Book Search lawsuit settlement:
    the parties to the lawsuit, along with the objectors, supporters, and friends-of-the-court,
    will be in the courtroom of United States District Judge Denny Chin offering oral
    arguments in the final settlement/fairness hearing. In his order, Judge Chin recognized
    26 parties that will speak for up to five minutes each on their positions in the
    settlement (21 in opposition, 5 in favor). The U.S. Department of Justice will
    also speak at the hearing. But I think we&#8217;re all eagerly awaiting to hear
    what the judge himself will say about the settlement agreement.In the lead-up
    to the hearing, Associate Professor James Grimmelmann at the New York Law School
    has continued his efforts, along with the students from the Institute for Information
    Law and Policy at New York Law School, to make the documents and proceedings of
    the lawsuit accessible and understandable to non-lawyers. In the most recent court
    filings leading up to Thursday&#8217;s hearing are some interesting nuggets.In
    his posting on the motion for attorneys fees, he notes that &#8220;counsel for
    the author sub-class are asking for the full $30 million in fees and reimbursement
    of their out-of-pocket costs.&#8221; The filing contains information about the
    number of hours and the billing rate for some of the lawyers working on the case.
    Some of the stuff is just really interesting, like one filing that included everything
    from 18 hours by a partner of a firm (who is also a law professor at NYU) at rate
    of $995/hour to an itemization of 51&cent; for long distance calls by the firm
    related to the case. Whew!More interesting to DLTJ readers would be Grimmelmann&#8217;s
    highlights of Dan Clancy&#8217;s declaration in support of the agreement. Dan
    Clancy is engineering director of the Google Book Search project, so he has a
    unique insight into the inner workings. Grimmlemann notes that Clancy states:To
    date, Google has Digitized over twelve million books, and intends to continue
    Digitizing books in the future.Google has received metadata from 48 libraries.Google
    pays approximately $2.5 million per year to license metadata from 21 commercial
    databases of information about books.Google has gathered 3.27 billion records
    about Books, and analyzed them to identify more than 174 million unique works.The
    third bullet is interesting in that I think we can eliminate one of the &#8220;commercial
    databases&#8221; from the list. I can&#8217;t find it in my notes from ALA Midwinter,
    but I seem to recall hearing Jay Jordan (OCLC President) say something along the
    lines that OCLC was not receiving a monetary return from the sharing of bibliographic
    data with Google; the value OCLC gets for its membership comes from the links
    back to WorldCat from Google services. If I got this wrong, I hope someone from
    OCLC will call me out on it.The last bullet is interesting, too: Google has identifying
    174 million works in analyzing all of the sources of data coming into it. I tried
    to find some numbers in the descriptions of WorldCat to compare that to, but didn&#8217;t
    have any luck this evening. (There isn&#8217;t anything about statistics available
    on http://worldcat.org/?)To Grimmelmann&#8217;s highlights I would add this statement
    that seems strangely out-of-place.Google has no interest in censorship. Indeed,
    Google&#8217;s mission is to organize the world&#8217;s information and make it
    universally accessible and useful.Has anyone brought censorship into the discussion
    yet? Privacy for sure, but censorship?Also:Google has developed algorithms to
    compare these numerous sources of metadata and identify the most accurate data
    about each book.They certainly seem to have invested a lot of effort in this area.
    More info can be found in my summary of Kurt Groetsch&#8217;s presentation at
    ALA Midwinter 2010. [...]"
- id: 44876
  author: CENTRAL TECHNICAL SERVICES TSI 2010 (ctsyell10)
  author_email: ''
  author_url: http://staff.library.wisc.edu/dept/cts/ctsyell10.htm
  date: '2010-02-18 09:22:39 -0500'
  date_gmt: '2010-02-18 14:22:39 -0500'
  content: "<!--%kramer-ref-pre%-->[...] I have seen two write ups on the program,
    so again, rather than writing up my own notes, here at the links to their write
    ups. Eric Hellman&#39;s blog is at:  http://go-to-hellman.blogspot.com/2010/01/google-exposes-book-metadata-privates.html
    \ and by Disruptive Library Technology Jester:  http://dltj.org/article/mashups-of-bib-data/
    [...]<!--%kramer-ref-post%-->"
- id: 45132
  author: Monday, January 18 - Presentations
  author_email: ''
  author_url: ''
  date: '2010-02-21 06:09:49 -0500'
  date_gmt: '2010-02-21 11:09:49 -0500'
  content: <!--%kramer-ref-pre%-->[...] 2/12/10:Peter Murray blogged this session
    on <a href="/article/mashups-of-bib-data/">Disruptive Library Technology
    Jester</a>, as did <a [...]<!--%kramer-ref-post%-->
- id: 91892
  author: Google Scholar linking open access pre-prints to citations? &laquo; Bibliographic
    Wilderness
  author_email: ''
  author_url: http://bibwild.wordpress.com/2010/10/04/google-scholar-linking-open-access-pre-prints-to-citations/
  date: '2010-10-05 10:55:15 -0400'
  date_gmt: '2010-10-05 14:55:15 -0400'
  content: "<!--%kramer-ref-pre%-->[...] (For a refresher on what they do with bibliographic
    data, take a look at my summary from Midwinter: Mashups of Bibliographic Data:
    A Report of the ALCTS Midwinter Forum.)              Click here to cancel [...]<!--%kramer-ref-post%-->"
- id: 97602
  author: 'Thursday Threads: RDA Revolt, Google Book Search Algorithm, Google Helps
    Improve Web Servers, Google&#8217;s Internet Traffic Hugeness | Disruptive Library
    Technology Jester'
  author_email: ''
  author_url: http://dltj.org/article/thursday-threads-2010w44/
  date: '2010-11-04 12:01:30 -0400'
  date_gmt: '2010-11-04 16:01:30 -0400'
  content: "[...] have the same inter-page linking hints that drive the PageRank algorithm
    for web search. The use of anonymized circulation data in creating clustered bibliographic
    descriptions was mentioned at the ALA Midwinter ALCTS Forum on [...]"
- id: 97806
  author: Google Books Getting Circulation Data from Partner Libraries | The Harvard
    Library Innovation Laboratory
  author_email: ''
  author_url: http://librarylab.law.harvard.edu/blog/2010/11/05/google-books-getting-circulation-data-from-partner-libraries/
  date: '2010-11-05 13:17:58 -0400'
  date_gmt: '2010-11-05 17:17:58 -0400'
  content: "[...] the Disruptive Library Technology Jester, certain libraries are
    providing Google Books with anonymized circulation data to feed into their relevance
    ranking algorithm.&nbsp; I would love to know which libraries are [...]"
- id: 159755
  author: thelibrarynews
  author_email: ''
  author_url: http://twitter.com/thelibrarynews/status/17788331442044928
  date: '2010-12-23 03:47:23 -0500'
  date_gmt: '2010-12-23 08:47:23 -0500'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">Mashups of Bibliographic Data: A Report of the
    ALCTS Midwinter Forum | Disruptive Library Technology Jester:  http://bit.ly/dYMivZ</span></span>'
- id: 159756
  author: Kazu.A
  author_email: ''
  author_url: http://twitter.com/kzakza/status/14251213834952704
  date: '2010-12-13 09:32:08 -0500'
  date_gmt: '2010-12-13 14:32:08 -0500'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">GoogleのKurt Groetsch氏によるGoogle Booksのメタデータ処理の紹介。先のブログと同じイベント。
    / Mashups of Bibliographic Data: A Report of the ALCTS M&hellip; http://htn.to/shiny6</span></span>'
- id: 159757
  author: Kazu.A
  author_email: ''
  author_url: http://twitter.com/kzakza/status/14166402017529856
  date: '2010-12-13 03:55:08 -0500'
  date_gmt: '2010-12-13 08:55:08 -0500'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">RT @minhavoz: @kzakza ちと古いですがこんな記事が http://bit.ly/8tpYhG
    もう少し詳しい記録 http://bit.ly/dfDBzy あと「SEO対応のための指南が出始めている」と触れられているのはこちらの記事でしょうか http
    ...</span></span>'
- id: 159758
  author: minhavoz
  author_email: ''
  author_url: http://twitter.com/minhavoz/status/14026561690796034
  date: '2010-12-12 18:39:27 -0500'
  date_gmt: '2010-12-12 23:39:27 -0500'
  content: <span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">@kzakza ちと古いですがこんな記事が http://bit.ly/8tpYhG もう少し詳しい記録
    http://bit.ly/dfDBzy あと「SEO対応のための指南が出始めている」と触れられているのはこちらの記事でしょうか http://oreil.ly/h1dfjk</span></span>
- id: 159759
  author: seijuro shimo
  author_email: ''
  author_url: http://twitter.com/sseijuro/status/25325748994
  date: '2010-09-23 17:15:39 -0400'
  date_gmt: '2010-09-23 21:15:39 -0400'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">RT @lichfieldgarden: 多分ID番号有無の事ですかね。古い和書はISBNないのでネット上では困る訳ですが
    @minoguchi: @sseijuro @lost_and_found GoogleBooksの書誌メタ関連 http://bit.ly/bZTwAR</span></span>'
- id: 159760
  author: seijuro shimo
  author_email: ''
  author_url: ''
  date: '2010-09-23 17:11:01 -0400'
  date_gmt: '2010-09-23 21:11:01 -0400'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">RT @minoguchi: .@sseijuro @lost_and_found まだちゃんと読めていないのですが、GoogleBooksの書誌メタデータ関連、この辺ちょっと参考になりそうかも、です
    // http://bit.ly/bZTwAR</span></span></span>'
- id: 159761
  author: 井野口 正之
  author_email: ''
  author_url: http://twitter.com/minoguchi/status/25314556262
  date: '2010-09-23 15:00:19 -0400'
  date_gmt: '2010-09-23 19:00:19 -0400'
  content: <span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">なるほど。ライブラリ・プロジェクトの図書とかISBNないもの山ほどありそうですからね&hellip;
    RT @lichfieldgarden 多分ID番号有無の事ですかね。古い和書はISBNないのでネット上では困る訳ですが RT @minoguchi http://bit.ly/bZTwAR</span></span>
- id: 159762
  author: 豊月
  author_email: ''
  author_url: http://twitter.com/yutuki_r/status/25314166126
  date: '2010-09-23 14:55:52 -0400'
  date_gmt: '2010-09-23 18:55:52 -0400'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">RT @lichfieldgarden: 多分ID番号有無の事ですかね。古い和書はISBNないのでネット上では困る訳ですが。RT
    @minoguchi: .@sseijuro @lost_and_found GoogleBooksの書誌メタデータ関連、この辺ちょっと参考に http://bit.ly/bZTwAR</span></span>'
- id: 159763
  author: ろす
  author_email: ''
  author_url: http://twitter.com/lost_and_found/status/25314041898
  date: '2010-09-23 14:54:26 -0400'
  date_gmt: '2010-09-23 18:54:26 -0400'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">RT @lichfieldgarden: 多分ID番号有無の事ですかね。古い和書はISBNないのでネット上では困る訳ですが。RT
    @minoguchi: .@sseijuro @lost_and_found GoogleBooksの書誌メタデータ関連、この辺ちょっと参考に http://bit.ly/bZTwAR</span></span>'
- id: 159764
  author: ろす
  author_email: ''
  author_url: http://twitter.com/lost_and_found/status/25314029432
  date: '2010-09-23 14:54:17 -0400'
  date_gmt: '2010-09-23 18:54:17 -0400'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">RT @minoguchi: .@sseijuro @lost_and_found まだちゃんと読めていないのですが、GoogleBooksの書誌メタデータ関連、この辺ちょっと参考になりそうかも、です  Mashups
    of Bibliographic Data http://bit.ly/bZTwAR</span></span>'
- id: 314151
  author: NT-ASIST
  author_email: ''
  author_url: http://twitter.com/nt_asist/status/252462311724437504
  date: '2012-09-30 17:38:00 -0400'
  date_gmt: '2012-09-30 21:38:00 -0400'
  content: '<span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">Mashups of Bibliographic Data: A Report of the
    ALCTS Midwinter Forum http://t.co/jpur4VbT</span></span>'
---
<p>This year the <a href="http://connect.ala.org/node/91406" title="ALCTS Forum: Mix and Match: Mashups of Bibliographic Data | ALA Connect"><acronym title="Association for Library Collections and Technical Services">ALCTS</acronym> Forum at <acronym title="American Library Association">ALA</acronym> Midwinter</a> brought together three perspectives on massaging bibliographic data of various sorts in ways that <em>use</em> <acronym title="Machine Readable Cataloging">MARC</acronym>, but where MARC is not the end goal.  What do you get when you swirl MARC, <acronym title="ONline Information eXchange">ONIX</acronym>, and various other formats of metadata in a big pot?  Three projects:  ONIX Enrichment at OCLC, the Open Library Project, and Google Book Search metadata.<br />
<!--more--><br />
Below is a summary of how these three projects are messin' with metadata, as told by the Forum panelists.  I also recommend reading Eric Hellman's <a href="http://go-to-hellman.blogspot.com/2010/01/google-exposes-book-metadata-privates.html" title="Google Exposes Book Metadata Privates at ALA Forum | Go-to-Hellman">Google Exposes Book Metadata Privates at ALA Forum</a> for his recollection and views of the same meeting.</p>
<h2 id="post-1478-h2-OCLC-ONIX">ONIX Enrichment at OCLC</h2>
<p><span class="removed_link" title="http://www.oclc.org/speakers/bios/register_renee.htm">Renee Register</span>, Global Product Manager for OCLC Cataloging and Metadata Services, was the first to present on the panel.  Her talk looked at a new and evolving product at OCLC on the enhancement of ONIX records with WorldCat records, and vice versa.  ((For those not familiar with <a href="http://www.editeur.org/8/ONIX/" title="ONIX Overview">ONIX</a>, it is a suite of standards promulgated by <a href="http://www.editeur.org/" title="EDItEUR homepage" rel="homepage">EDItEUR</a> for the interchange of information on books and serial publications.  It is primarily used as the communication channel between the publishing industry through distribution chains to retail establishments.))</p>
<p>As libraries, Renee said "our instincts are collaborative" but "our data and workflow silos encourage redundancy and inhibit interoperability."  Beyond the obvious differences in metadata formats, the workflows of libraries differ dramatically from other metadata providers and consumers. In libraries (with the exception of <acronym title="Cataloging in Print">CIP</acronym> and brief on-order records) the major work of bibliographic production is performed at the end of the publication cycle and ends with the receipt of the published item.  In the publisher supply chain, bibliographic data evolves over time, usually beginning months before publication and continuing to grow for months and years (sales information, etc.) after publication.  Renee had a graphic showing the current flow of metadata around the broader bibliographic universe that highlighted the isolation of library activity relative to publisher, wholesaler, and retailer activity.</p>
<p>[caption id="attachment_1484" align="alignright" width="300" caption="Diagram of the Process of Enhancing ONIX Records, from OCLC Services for the Publisher Supply Chain Webinar, August 2009"]<a href="http://www5.oclc.org/downloads/presentations/MDS4Pubs_August_Webinar_200908.ppt" title="Slides from Publisher Supply Chain Webinar, August 2009"><img src="/wp-content/uploads/2010/01/ONIX-enhancement-300x225.jpg" alt="" title="Diagram of the Process of Enhancing ONIX Records" width="300" height="225" class="size-medium wp-image-1484" /></a>[/caption]Renee when on to describe a "next generation cataloging data flow" where OCLC facilitates the inclusion of publisher data into <a href="http://www.worldcat.org/" title="WorldCat homepage" rel="homepage">WorldCat</a> and enhances publisher data with information extracted from WorldCat.  To the right is a version of the graphic she used at Midwinter taken from an earlier presentation on the same topic.  It show ONIX-formatted metadata coming into WorldCat, being cross-walked and matched with existing MARC data in WorldCat, and finally extracted and cross-walked back to ONIX resulting in <a href="http://publishers.oclc.org/en/metadata/default.htm" title="OCLC Metadata Services for Publishers"> enhanced ONIX metadata</a> for publishers to use in their supply chain.  If there is an exact match for the incoming ONIX record in WorldCat, the WorldCat record is enhanced with certain fields from the ONIX record (descriptions, author biographies, web links) -- being careful not to override authority work being done by libraries, but adding enhancements that libraries may not otherwise input.  In turn, enhancements from exact match record and FRBR work set records (hardcover versus softcover versus audiobook, etc.) are added to the ONIX record (non-English subject headings, adding a Dewey Decimal Classification (DDC) field from another similar record if one doesn't already exist, change the author field to an authority-controlled version).  If there is not an exact match for the ONIX record in WorldCat, a new WorldCat record is built from the ONIX record and it is subsequently enhanced by metadata found in the FRBR work set records.  In doing so, we are "increasing the goodness of metadata in the marketplace," as Renee put it in her presentation.  OCLC is also creating a mapping between <a href="http://web.archive.org/web/20131412555500/http://www.bisg.org/what-we-do-20-73-bisac-subject-headings-2009-edition.php" title="Standards &amp; Best Practices | Classification Schemes | BISAC Subject Headings 2009 Edition | Book Industry Study Group">BISAC Subject Headings</a> ((By the way, it seems like BISAC is an acronym for "Book Industry Systems Advisory Committee", the former name of the <a href="http://www.bisg.org/" title="Book Industry Study Group homepage" rel="homepage">Book Industry Study Group</a>.)) and the DDC system.  This allows the enhancement of ONIX with suggestions of BISAC Subject Terms and the enhancement of WorldCat records with generic DDC fields given an incoming BISAC Subject Term value from the ONIX record.</p>
<p>In her experience, Renee said that libraries need ways to enable our metadata to evolve over time and allow for publisher-created metadata to merge effectively with library-created metadata.  The bibliographic record needs to be a "living, growing" thing throughout the lifecycle of a title and beyond.  In concluding her remarks, she offered several resources to explore for further information:  the OCLC/NISO study on <a href="http://www.niso.org/publications/white_papers/StreamlineBookMetadataWorkflowWhitePaper.pdf" title="Streamlining Book Metadata Workflow">Streamlining Book Metadata Workflow</a>, the U.K. Research Information Network report on <a href="http://rin.ac.uk/creating-catalogues" title="Creating Catalogues: Bibliographic Records in a Networked World">Creating Catalogues: Bibliographic Records in a Networked World</a>, the Library of Congress <a href="http://www.loc.gov/bibliographic-future/news/" title="News, Press Releases and Reports - Working Group on the Future of Bibliographic Control (Library of Congress)">Study of the North American MARC Records Marketplace</a>, the Library of Congress <a href="http://www.loc.gov/publish/cip/topics/onixpro.html" title="LC ONIX Pilot Project">CIP/ONIX Pilot Project</a>, and the <a href="http://publishers.oclc.org/en/default.htm" title="OCLC Publisher Supply Chain Website">OCLC Publisher Supply Chain Website</a>.</p>
<h2 id="post-1478-h2-Open-Library">From MARC to Wiki with Open Library</h2>
<p>The second presenter on the panel was <a href="http://kcoyle.net/" rel="homepage" title="Karen Coyle's home page">Karen Coyle</a>, talking about the mashup of metadata at the <a href="http://openlibrary.org/" title="Open Library project homepage" rel="homepage">Open Library</a> project at the <a href="http://archive.org/" title="Internet Archive homepage" rel="homepage">Internet Archive</a>.  The slides from her presentation are <a href="http://kcoyle.net/presentations/ol_boston.pdf" title="Open Library - Mix and Match Metadata presentation slides [PDF]">available from her website</a>.</p>
<p>Karen said right at the start that the Open Library project is different from most of what happens in libraries -- it is "someone outside the library world making use of library data" -- although the goal is arguably the same as others -- "<a href="http://openlibrary.org/about" title="About Us (Open Library)">One web page for every book ever published</a>."  As such, the Open Library isn't a library catalog as librarians think of it in that it is not a representation of a libraries inventory. It has metadata for every book it can know about and a pointer to places where the book can be found, including all of the electronic books in Internet Archive (<a href="http://www.opencontentalliance.org/" rel="homepage" title="Open Content Alliance (OCA)">Open Content Alliance</a>, Google Public Domain, etc.) as well as pointers back to OCLC WorldCat.  Karen's role for the project is that of "Library Data Informant." The Internet Archive decided that they needed someone who understood library data in order to try to use it.  From Karen's perspective, she is trying to be a resource for project but not give them any guidance on how to implement the service.  She is curious to see what the project would do when bibliographic data is viewed from a non-librarian perspective.  If they have questions, or if they have assumptions about data that are wrong, then she intervenes.</p>
<p>Karen went on to briefly describe the Open Library system.  Open Library doesn't have records; rather, it has field types and data properties.  In this way, it uses semantic web concepts.  "Author" is a type, "Author birthdate" is another type, and so forth.  There are no set field types, so if the project gets data from source for which a type doesn't yet exist, it can create a new one.  Each type can have data properties such as string, boolean, text, link, etc.  Nothing is required and everything is repeatable.  Everything -- types, properties, and values -- gets a <acronym title="Uniform Resource Identifier">URI</acronym> (a URI is an identifier like a URL, but conceptually a superset of the universe of URLs).  Titles, authors, subjects, author birthdates, and so on have URIs.  Lastly, the underlying data structures are based on wiki principles: all edits are saved and viewable, anyone can edit any value, anyone can add new types or properties, anyone can develop their own displays, etc.</p>
<p>The data that is now in Open Library came from a variety of sources.  They started with a copy of books from the Library of Congress, and continue to receive the weekly updates. They performed a crawl of Amazon's book data.  They have gotten some from publishers, libraries, and individual users.  The last is perhaps the most interesting because it is mainly people outside the western world who are otherwise having trouble getting their works recognized.</p>
<h3 id="post-1478-h3-Problems-Issues">Problems, Issues, Challenges, and Opportunities with the Data</h3>
<p>People who use library data without the biases or assumptions of librarians come up with interesting ways to view the data.  Karen described a few of them.</p>
<dl class="inlineClass">
<dt>Names -</dt>
<dd>"These library forms of names? Honestly no one but us can stand them."  Even something as simple as the form of last-name-comma-first-name is troublesome.  No one else uses this form of the name: Amazon, Wikipedia, etc.  In processing these, any information between parenthesis has been deleted, birth and death dates move into separate field types.</dd>
<dt>Titles -</dt>
<dd>In working with the Open Library developers, this is one place that Karen tried insisting on applying a library practice:  knowing the initial article.  For us, this is important for sorting books in alphabetical order.  The developer response -- why do we have to sort in alphabetical order?  "Where else but library catalogs to we see things sorted in alphabetical order?  Not in Google, not in Amazon, not anywhere.  Alphabetical order is not in the mindset anymore."  They also found that the title might include extraneous data.  Amazon, for instance, appends the series title in parenthesis to the main title.  This is a demonstration of how other communities are not as concerned about strongly typing and separating information into fields. Amazon, of course, has reasons for series information into the main title: it helps sell books.</dd>
<dt>Product dimensions -</dt>
<dd>Publishers and distributors need to know characteristics of an item such as height, width, depth, and weight; they, of course, need to put it in a box and ship it.  Libraries, concerned about placing the item on the shelf, record just height.  Recording pagination is different, too: libraries use odd notations "ill. (some col)" and "xv, 200p." versus simply "200 pages."</dd>
<dt>Birthdates -</dt>
<dd>Librarians use birthdates to distinguish names; if there is no need to distinguish a name, birth and death dates are not added.  Someone looking at this from the outside would ask 'Why don't all authors have birth and death dates?'  This can be useful information for viewing the context of an item, not just to distinguish author names.  Open Library ran author names against Wikipedia to pick up not only birth and death years, but also the actual dates.</dd>
<dt>Subject headings -</dt>
<dd>Open Library using Library of Congress Subject Headings was out of the question. In processing the data, the Open Library developers just broke them apart into segments and used them. But because they were able to do data mining on the subject field types, they did find statistical relationships between the disassembled precoordinated headings and were able to present those to the user.</dd>
<dt>The View of the Data -</dt>
<dd>Rather than a traditional library view of long lists of author-title, the Open Library (in its next version coming in February) will have several different views into the mass of data: Authors; Books (what we would call <acronym title="Functional Requirements for Bibliographic Records">FRBR</acronym> 'manifestations'); Works; Subjects; and eventually places, publishers, etc.  For example, when searching for an author one would get the author page.  On it would be all of the works from the author as well as other biographical information.  It looks similar to a WorldCat identities page, except it is the actual user interface built into the system.  Similarly, every work will have a page, and at the bottom of it one will see all of the editions of the work.  Also, each subject will have a page, and one will see a list of works with that subject as well as authors who write on that subject.  As Karen said, "The subject itself becomes an object of interest in the database, not just something that is just tacked on to the bottom of the library record."</dd>
<dt>Data mining -</dt>
<dd>With the data in this format, it is possible to perform data mining actions against it. For instance, simple data mining such as country of publication, popular places that appear, etc.  When they had the problem of author names -- knowing when to reverse surname and forname -- they ran the names against Amazon and Wikipedia and retained the ones where they found the order of the entry was the same. The Open Library developers are also experimenting with data mining to find publisher names.  Publisher names, of course, vary dramatically, but by using ISBN prefixes they can pull together related items into a "publisher" view.</dd>
</dl>
<p>Karen suggested watching the <a href="http://edwardbetts.com/ol/" title="Index of /ol">Edward Betts's site</a>, one of the developers of the Open Library project with an eye on the data mining aspects.  She said it is fun to look at our data when it can be viewed from this different point-of-view.  She also said to watch out for a new version of the <a href="http://openlibrary.org/" title="Open Library (Open Library)">Open Library website</a> coming in February.</p>
<h2 id="post-1478-h2-Google-Book-Search-Metadata">Google Book Search Metadata</h2>
<p>The final presenter was <a href="http://web.archive.org/web/20100123004512/http://www.google.com:80/profiles/kurt.groetsch" title="Kurt Groetsch&#039;s Google Profile">Kurt Groetsch</a>, Technical Collections Specialist at Google where he works to provide understanding and insight into library partner collections and the digitized books from Google.  Kurt said that "Google has been fairly circumspect over the years about what we do on the Book Search project."  He said it was a bit of a cultural legacy from the rest of the company and also possibly an artifact of the copyright litigation, but he is hoping to change that.  His presentation looked at how Google works with book metadata from three vantage points -- the inputs into Google's system, parsing by Google's algorithms, and analysis and output into the public interfaces.</p>
<p>On the input side, Google is getting bibliographic metadata from over 100 sources in a variety of formats. MARC records are coming from libraries, union catalogs, commercial providers (OCLC), publishers/retails (one publisher supplies records in MARC format).  Google also gets ONIX records from commercial providers (such as Ingram and Bowker), publishers, and retailers.  Google is especially interested in data from non-U.S. retailers because it is a source of information about books published outside the United States; it helps facilitate discovery of items that they may not otherwise encounter in the <a href="https://books.google.com/partner/">publisher</a> and <a href="http://www.google.com/googlebooks/library.html" title="Google Books Library Project">library</a> programs.  Google also receives records in a variety of "idiosyncratic formats" -- for example, publisher-contributed metadata (via the Publisher Partner Program); information associating books with jacket images; name authority records (from LC); reviews; popularity signals (sales data as well as <a name="anonymized_circulation_data">anonymized circulation data</a> from some library partners, useful for feeding into the relevancy ranking algorithm); and internally-generated metadata (for instance, whether a book is commercially available or not).  Google processes all of this information to come up with a single record that describes a book.  At this point they have over 800 million bibliographic records and one trillion bits of information in those records.</p>
<p>All of these records from all of these sources are processed and remixed with Google's parsing algorithms about twice a week.  The first step is to transform the incoming records into a "less verbose format" for storage and processing.  It is a SQL-like structure that allows elements of the metadata to be queried.  Records are then parsed to extract specific bits of information, transform the bits as necessary, and write the information to an internal "resolved records" data structure (a subset of the data coming from the input formats).  In the presentation, Kurt had examples of how making inferences from data coming from both MARC and ONIX can be troublesome.  Parsing also involves extracting "bibkeys" from the records to aid in matching across sources of data.  Four types of identifiers are extracted from bibliographic records: OCLC numbers, <acronym title="Library of Congress Control Numbers">LCCN</acronym>s, ISBNs, and ISSNs.  They provide usually useful signals when matching bibliographic and help with assertions that two records describe the same manifestation.  Google also tries to parse item data when present in records representing multi-volume works, enumeration and chronology.  They will also treat barcode as a form of a "bibkey" if they get it from a library.  The parsing algorithm will also split records containing multiple ISBNs representing different product forms (e.g. hardback, paperback, etc.).</p>
<p>With all of this data parsed into records, Google starts its clustering process where records are examined and attached to each other.  Bibkeys provide significant evidence for relating records to each other, but bibkeys are not always present in a record (non-U.S. records and older records frequently contain no bibkeys).  The algorithms then fall back on text similarity matching using title, subtitle, contributor and other fields such as publisher and publication year.  The results are clusters of records representing the same manifestation. An algorithm then attempts to derive the "best-of" record for a single cluster from all of the parsed input records.  This is done in a field-by-field voting process based on the trustworthiness of individual fields from record sources.</p>
<p>Kurt went into some of the challenges facing the team building the clustering and best-of record creation algorithms.  For instance, in dealing with multivolume works they know of 5 numbering schemas with 3 number types in 15 different languages.  Enumeration is now showing in the public display, but the development team is still working with unparsable item data due to inconsistent cataloging practices between institutions...and sometimes inconsistencies within an institution.  Another problem is non-unique identifiers. In the current data set ISBN 7899964709 is shared by 75 books and ISBN 7533305353 is associated with 1413 books. There are also poor quality or "junk records".  Kurt said his favorite was "The Mosaic Navigator" by Sigmund Freud published in 1939.  These are hard to identify with an algorithm, and they rely on reports of problems that enable the developers to go in and "kill" the troublesome record.  Another example is a book by Virginia Woolf where the incoming record had conflicting information; it had two 260 fields that contained different dates (1961, correct, and 1900) with fixed field information that strongly suggested that 1900 was the single date of publication.  When the data problem is systematic, they can identify it and compensate for it.  Kurt's example for this case was "The United States Since 1945" published in 1899.  This one was highlighted in <a href="http://chronicle.com/article/Googles-Book-Search-A/48245/" title="Google's Book Search: A Disaster for Scholars - The Chronicle Review - The Chronicle of Higher Education">Geoffrey Nunberg's criticism of Google Books metadata</a>.  In this case, there was a source of metadata from Brazil that when they didn't know the date of publication would use 1899.  When Google went back and looked at the date distribution of books there was a huge spike in 1899.  Once Google knew about it they were able to go in and kill that information from that source of records.  ((A side note: Google isn't the only one tripped up by this.  If one searches for the ISBN of the item, 0195038487, you get to <a href="http://www.biggerbooks.com/book/9780195038484" title="The United States Since 1945 at BiggerBooks.com -  Leuchtenburg, 9780195038484, History">more</a> <a href="http://www.chegg.com/details/the-united-states-since-1945/0195038487/" title="Chegg.com: The United States Since 1945 by Leuchtenburg">than</a> <a href="http://www.amazon.co.uk/The-United-States-Since-1945/dp/0195038487" title="The United States Since 1945: Amazon.co.uk: Books">one</a> site that has the same incorrect publication date.  At least Google is attempting to clean up the data!))</p>
<p>In closing, Kurt said that Google is committed to engaging with the library community on improving metadata and metadata processing.</p>
<p style="padding:0;margin:0;font-style:italic;">The text was modified to update a link from http://www.niso.org/publications/white_papers/Stream lineBookMetadataWorkflowWhitePaper.pdf to http://www.niso.org/publications/white_papers/StreamlineBookMetadataWorkflowWhitePaper.pdf on January 19th, 2011.</p>
<p style="padding:0;margin:0;font-style:italic;" class="removed_link">The text was modified to remove a link to http://www.oclc.org/speakers/bios/register_renee.htm on February 11th, 2011.</p>
<p style="padding:0;margin:0;font-style:italic;">The text was modified to update a link from http://cip.loc.gov/onixpro.html to http://www.loc.gov/publish/cip/topics/onixpro.html on November 13th, 2012.</p>
