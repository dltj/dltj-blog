---
title: 'Issue 97: Again with the AI Chatbots'
modified: 2023-01-25T21:41:24-05:00
category: Thursday Threads
categories:
- Thursday Threads
tags:
- large language model
- scholarly publishing
- generative artificial intelligence
- educational technology
mastodon:
- #LargeLanguageModels such as those used in ChatGPT are become a very thick thread in recent issues of DLTJ Thursday Threads. This week's issue has four threads—with two updates from previous threads—that cover this topic. Read on to hear more... https://dltj.org/article/issue-97-large-language-models 1/5
- Two weeks ago we learned that CNET was using #LargeLanguageModels to generate monotonous, click-baity articles. ("What is Zelle and How Does It Work" anyone?) This week, the source that broke that story uncovered how bad the practice really was. https://dltj.org/article/issue-97-large-language-models#cnet 2/5
- Beyond the mainstream press, #LargeLanguageModels are being used in #ScholarlyCommunication now, and #SpringerNature is trying to set some rules on the use of #ChatGPT and similar tools in journal articles https://dltj.org/article/issue-97-large-language-model#scholarly-publishing 3/5
- Sticking with a #HigherEducation theme, #ChatGPT came to common usage very fast and instructors are still trying to cope. What does homework look like when #LargeLanguageModels are easily available? https://dltj.org/article/issue-97-large-language-model#higher-education 4/5
- Beyond higher education, how does society adapt to #AI...opportunities (new types of jobs) and challenges (detecting when AI is used where it should be). https://dltj.org/article/issue-97-large-language-model#adapting 5/5
- And the weekly addition to #CatsOfMastodon. Alan desperately wants to be an outdoor cat, but that is hard when there is snow on the ground. bonus/5
---
The hot technology in the news now is chatbots driven by artificial intelligence. 
(This specific field of artificial intelligence is "large language models" or LLM). 
There were two LLM threads in <i>DLTJ Thursday Threads</i> [issue 95](https://dltj.org/article/issue-95-chatgpt-cryptomining-wssl) and a whole issue six weeks ago ([issue 93](https://dltj.org/article/issue-93-ai-chat/)). 
I want to promise that <i>Thursday Threads</i> will not turn into an every-other-issue-on-LLMs, but so far that is what is catching my eye here at the start of 2023.

- [AI generating news articles, and we're not impressed](https://dltj.org/article/issue-97-large-language-models#cnet) (updating issue 95's [AI generating news articles](https://dltj.org/article/issue-95-chatgpt-cryptomining-wssl#ai-news-generator))
- [Large language models in scholarly publishing](https://dltj.org/article/issue-97-large-language-model#scholarly-publishing)
- [Higher education classrooms prepare for ChatGPT](https://dltj.org/article/issue-97-large-language-model#higher-education)
- [Adapting to the disruptions of generative artificial intelligence](https://dltj.org/article/issue-97-large-language-model#adapting)

Also on <i>DLTJ</i> in the past week:

- [LibNFT: a second look…still ‘nope’](https://dltj.org/article/libnft-2/)

{% include thursday-threads-header.html %}

## AI generating news articles, and we're not impressed
{: #cnet}
{% include thursday-threads-quote.html
blockquote='<p>CNET will pause publication of stories generated using artificial intelligence “for now,” the site’s leadership told employees on a staff call Friday.</p><p>The call, which lasted under an hour, was held a week after CNET came under fire for its use of AI tools on stories and one day after The Verge reported that AI tools had been in use for months, with little transparency to readers or staff. CNET hadn’t formally announced the use of AI until readers noticed a small disclosure.</p><p>“We didn’t do it in secret,” CNET editor-in-chief Connie Guglielmo told the group. “We did it quietly.”</p>'
href="https://www.theverge.com/2023/1/20/23564311/cnet-pausing-ai-articles-bot-red-ventures"
versionurl="https://web.archive.org/20230124040930/https://www.theverge.com/2023/1/20/23564311/cnet-pausing-ai-articles-bot-red-ventures"
versiondate="2023-01-23T23:09:11"
anchor="CNET pauses publishing AI-written stories after disclosure controversy"
post=', The Verge, 20-Jan-2023'
%}

This is the end, for now, of a saga that started when {{ robustlink(href="https://futurism.com/the-byte/cnet-publishing-articles-by-ai", versionurl="https://web.archive.org/web/20230111184722/https://futurism.com/the-byte/cnet-publishing-articles-by-ai", versiondate="2023-01-11", title="CNET Is Quietly Publishing Entire Articles Generated By AI | Futurism", anchor="Futurism") }} found that CNET was using a "CNET Money Staff" byline for articles being generated with an in-house large-language-model (LLM) AI system. 
(That was covered in <i>Thursday Threads</i> [issue 95](https://dltj.org/article/issue-95-chatgpt-cryptomining-wssl#ai-news-generator).) 
CNET was using the tech to create monotonous articles like "Should You Break an Early CD for a Better Rate?" or "What is Zelle and How Does It Work?" 
That might have been the end of it if human editors had indeed proofread the articles before publication (as CNET had claimed). 
Either the editors were bad at their job or {{ robustlink(href="https://futurism.com/cnet-ai-errors", versionurl="https://web.archive.org/20230118030826/https://futurism.com/cnet-ai-errors", versiondate="2023-01-17T22:08:21", title="CNET's Article-Writing AI Is Already Publishing Very Dumb Errors | Futurism", anchor="it was not the case") }}. 
Oh, and CNET was using the LLM to {{ robustlink(href="https://futurism.com/cnet-ai-articles-label", versionurl="https://web.archive.org/web/20230120013125/https://futurism.com/cnet-ai-articles-label", versiondate="2023-01-20T00:08:37", title="CNET Secretly Used AI on Articles That Didn't Disclose That Fact, Staff Say | Futurism", anchor="rewrite the first few paragraphs of articles every couple of weeks") }} so that they would stay "fresh" in web search engine crawls. 
{{ robustlink(href="https://www.cnet.com/tech/cnet-is-experimenting-with-an-ai-assist-heres-why/", versionurl="https://web.archive.org/20230114051953/https://www.cnet.com/tech/cnet-is-experimenting-with-an-ai-assist-heres-why/", versiondate="2023-01-14T00:19:52", title="CNET Is Experimenting With an AI Assist. Here's Why | CNET", anchor="CNET admitted to using the LLM") }} before ultimately pausing the use of the technology (for now), as <i>The Verge</i> article describes.


## Large language models in scholarly publishing
{: #scholarly-publishing}
{% include thursday-threads-quote.html
blockquote='<p><i>Nature</i>, along with all Springer Nature journals, has formulated the following two principles, which have been added to our existing guide to authors (see go.nature.com/3j1jxsw). As <i>Nature</i>’s news team has reported, other scientific publishers are likely to adopt a similar stance.</p><p>First, no LLM tool will be accepted as a credited author on a research paper. That is because any attribution of authorship carries with it accountability for the work, and AI tools cannot take such responsibility.</p><p>Second, researchers using LLM tools should document this use in the methods or acknowledgements sections. If a paper does not include these sections, the introduction or another appropriate section can be used to document the use of the LLM.</p>'
href="https://www.nature.com/articles/d41586-023-00191-1"
versionurl="https://web.archive.org/web/20230125072621/https://www.nature.com/articles/d41586-023-00191-1"
versiondate="2023-01-25T19:08:24"
anchor="Tools such as ChatGPT threaten transparent science; here are our ground rules for their use"
post=' (editorial), Nature, 24-Jan-2023'
%}

The Springer Nature publisher has set some early ground rules, even as it admits that ultimately it may not be able to judge whether a large-language-model (LLM) had been used in the crafting of an article. 
Last week <i>Nature</i> noted that at least {{ robustlink(href="https://www.nature.com/articles/d41586-023-00107-z", versionurl="https://web.archive.org/20230120141730/https://www.nature.com/articles/d41586-023-00107-z", versiondate="2023-01-19T23:08:14", title="ChatGPT listed as author on research papers— many scientists disapprove | Nature", anchor="four articles had already been submitted citing ChatGPT as a co-author") }}.


## Higher education classrooms prepare for ChatGPT
{: #higher-education}
{% include thursday-threads-quote.html
blockquote='<p>Mr. Aumann confronted his student over whether he had written the essay himself. The student confessed to using ChatGPT, a chatbot that delivers information, explains concepts and generates ideas in simple sentences — and, in this case, had written the paper.</p><p>Alarmed by his discovery, Mr. Aumann decided to transform essay writing for his courses this semester. He plans to require students to write first drafts in the classroom, using browsers that monitor and restrict computer activity. In later drafts, students have to explain each revision. Mr. Aumann, who may forgo essays in subsequent semesters, also plans to weave ChatGPT into lessons by asking students to evaluate the chatbot’s responses.</p>'
href="https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial-intelligence-universities.html"
versionurl="https://web.archive.org/20230117005038/https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial-intelligence-universities.html"
versiondate="2023-01-16T13:08:42"
anchor="Alarmed by A.I. Chatbots, Universities Start Revamping How They Teach"
post=', New York Times, 16-Jan-2023'
%}

In [issue 93](https://dltj.org/article/issue-93-ai-chat/#ai-essays), I mentioned a high school teacher that was lamenting the disruption that large-language-models (LLMs) were having on the classic English essay (and mentioned Ben Thompson's suggestion of {{ robustlink(href="https://stratechery.com/2022/ai-homework/", versionurl="https://web.archive.org/20221213000811/https://stratechery.com/2022/ai-homework/", versiondate="2022-12-12T19:08:08", title="AI Homework | Stratechery", anchor="&ldquo;Zero Trust Homework&rdquo;") }} to combat LLMs).
This <i>New York Times</i> article describes more examples of how instructors are coping with LLMs.


## Adapting to the disruptions of generative artificial intelligence
{: #adapting}
{% include thursday-threads-quote.html
blockquote='Generative AI models for businesses threaten to upend the world of content creation, with substantial impacts on marketing, software, design, entertainment, and interpersonal communications. These models are able to produce text and images: blog posts, program code, poetry, and artwork. The software uses complex machine learning models to predict the next word based on previous word sequences, or the next image based on words describing previous images. Companies need to understand how these tools work, and how they can add value.'
href="https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work"
versionurl="https://web.archive.org/web/20230115205726/https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work"
versiondate="2023-01-15T17:11:08"
anchor="How Generative AI Is Changing Creative Work"
post=', Harvard Business Review, 14-Nov-2022'
%}
We will adapt to new technology (if history is any guide). 
This <i>Harvard Business Review</i> article is about more than the large-language-models discussed in this issue. 
(It also covers the generative adversarial network technology that creates "AI art".) 
But the lessons and cautions are generally applicable. 
If fact, we may see new professions emerging, like a ["prompt engineer"](https://hyp.is/eW__HJUXEe2vwGcfh597Hw/hbr.org/2022/11/how-generative-ai-is-changing-creative-work) that will know the phrasing and techniques to best elicit the output the client is seeking. 
(The article describes the efforts of an award-winning AI artist: "he spent more than 80 hours making more than 900 versions of the art, and fine-tuned his prompts over and over.")
Or, like was suggested with the "Zero Trust Homework" idea (and seemingly resoundingly ignored by the CNET editors), using LLM to "generate original content" faster so "writers now have time to do better research, ideation, and strategy."
We will also see these technologies used for "deepfakes" (in {{ robustlink(href="https://techcrunch.com/2022/08/12/a-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned/", versionurl="https://web.archive.org/20220819011436/https://techcrunch.com/2022/08/12/a-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned/", versiondate="2022-08-18T21:14:23", title="This startup is setting a DALL-E 2-like AI free, consequences be damned | TechCrunch", anchor="still images") }}, {{ robustlink(href="https://metaphysic.ai/to-uncover-a-deepfake-video-call-ask-the-caller-to-turn-sideways/", versionurl="https://web.archive.org/20220809210817/https://metaphysic.ai/to-uncover-a-deepfake-video-call-ask-the-caller-to-turn-sideways/", versiondate="2022-08-09T17:08:14", title="To Uncover a Deepfake Video Call, Ask the Caller to Turn Sideways | Metaphysic.ai", anchor="video") }}, and {{ robustlink(href="https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/", versionurl="https://web.archive.org/20230116010817/https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/", versiondate="2023-01-15T20:08:14", title="Microsoft’s new AI can simulate anyone’s voice with 3 seconds of audio | Ars Technica", anchor="audio") }}) and activities bordering on plagiarism (such as the earlier scholarly communication thread).

Come to think of it, this sort of thread is likely to be quite common in upcoming <i>DLTJ Thursday Threads</i> issues.


## Alan in snowy weather
{: #alan}
{{ image(width="700", localsrc="2023/2023-01-26-alan-snow.jpg", alt="Photograph of a white cat with black splotches sitting up tall on a paver patio. The ground in front of the cat is covered with snow.") }} 
We got snow in central Ohio, and Alan just had to check it out. 
You will note that he is not _in_ the snow, just _next to_ the snow. 
He has gone full "house-cat" after all.