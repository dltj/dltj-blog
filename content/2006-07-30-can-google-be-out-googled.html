---
layout: wordpress-import
status: publish
published: true
title: Can Google be Out-Googled?
modified: 2006-07-30T23:30:16+00:00
author: Peter Murray
author_login: lyrdor
author_email: jester@dltj.org
author_url: http://dltj.org/about
wordpress_id: 94
wordpress_url: http://dltj.org/2006/07/can-google-be-out-googled/
date: '2006-07-30 19:30:16 -0400'
date_gmt: '2006-07-31 00:30:16 -0400'
categories:
- Disruption in Libraries
tags:
- Amazon
- metadata
- library 2.0
- Google
- disruptive innovation
comments:
- id: 5335
  author: Sergio Berna
  author_email: sberna@gmail.com
  author_url: ''
  date: '2006-10-04 07:43:59 -0400'
  date_gmt: '2006-10-04 11:43:59 -0400'
  content: "First of all I would like to say that I enjoyed reading the article, it
    is very well written and summarizes in a very precise form the writers point of
    view about the problem.\r\n\r\nNevertheless it starts with a very interesting
    point that is not further followed. The main question is, Do you compete at all?.
    Or stated in a more exact form, do you need to compete at all?. In those 2 questions
    I want to imply that maybe, google is no competition for you.\r\n\r\nLets have
    a look at the Google and Amazon search engines example from another point of view.
    The user.\r\n\r\nWhat the user wants / needs is a solution for his problem, find
    lion king cupcakes. He doesn&rsquo;t really think about metadata at all. He just
    thinks about his problem and as such goes to massive text search engine like google.
    There, google, excelling at his core business directly redirects the user to the
    best place available to satisfy the user perceived need. Amazon.\r\n\r\nNo competition
    among Google and Amazon as such, and I don&rsquo;t think it is in the mind of
    neither of them to compete. Google is certainly not competing with Amazon since
    it is the first reference to happen. And I don&rsquo;t really think Amazon thinks
    Google is competing with him since Google is driving clients right to its grasp.\r\n\r\nLets
    get back to the user. In his mind there are two processes, the first process is
    oriented towards locating something that satisfies his need. The second process
    is directed towards obtaining that very same thing in the (easiest?, cheapest?,
    most secure?, just choose your adjective) way. There we see a collaboration in
    locating the subject and after that Google disappears and Amazon is all alone
    in finally satisfying the client. Is Amazon doesn&rsquo;t satisfy the client both
    Google and Amazon lose. On the other hand if the client leaves Amazon as a happy
    buyer both win.\r\n\r\nSo the question is, Why compete at all?. Google is not
    an specialized, categorizing, need oriented catalog. It is just a very smart search
    engine. In mi opinion it is so good at doing what it does because it presents
    the most simple interface ever shown. A simple textbox and a button. You cant
    simplify more without showing a blank page. The message is clear to the user:\r\n\r\n&ldquo;hey
    just tell me what you need in three or four words, ill do the first part of the
    road for you and will set you in the portal of the best specialist available&rdquo;\r\n\r\nThinking
    in a Google interface with combos and such simply breaks the Google concept (have
    any of you ever used the advanced search interface?). But you can&rsquo;t have
    specialized advice without those. In it is there where there is plenty of space
    to breathe and where I see lots of opportunities for cooperative-competing."
- id: 5452
  author: the jester
  author_email: jester@DLTJ.org
  author_url: http://dltj.org/
  date: '2006-10-06 15:57:53 -0400'
  date_gmt: '2006-10-06 19:57:53 -0400'
  content: "I think you may have missed the main thrust of the posting.  It is common
    rhetoric in the North American library community to make library services \"appear
    more like Google.\"  As you indicated in your comment, a simple search box that
    encourages the user to put in just three or four words about the information being
    sought is, if usability studies are to be believed, a very appealing interface
    that works well in stark contrast to the prototypical library service interface.\r\n\r\nRather,
    the main focus of the posting was how to make our complicated library service
    interfaces as appealing as Google yet deliver a better end result to users.  My
    argument was that the answer lies in faceted metadata, as demonstrated by the
    Amazon interface and, to a limited extent, the <a href=\"http://www.lib.ncsu.edu/catalog/\"
    rel=\"nofollow\">NCSU Libraries Catalog</a> interface.  Users can be presented
    a rich set of exploratory and limiting functions after the initial three or four
    words <em>if</em> the descriptive metadata exists to drive the interface creation.
    \ Amazon has that rich metadata, Google does not, and you can see the effects
    of that in how they present search results.\r\n\r\n(The \"limited extent\" comment
    with regards the the NCSU Libraries Catalog interface, by the way, refers to the
    fact that its interface is only useful for finding printed and bound aggregate
    volumes of material &mdash; otherwise known as \"books.\"  It is like the first
    Amazon interfaces that were limited to just books because that was the only thing
    in Amazon's database.  Through Amazon's interface, however, you can now discover
    a wide range of items, and the next generation discovery interface in libraries
    should find not only books, but article, maps, pictures, datasets, websites, and
    other relevant material...all from the same simple search box.)"
- id: 5521
  author: Sergio Berna
  author_email: sberna@gmail.com
  author_url: ''
  date: '2006-10-09 06:11:28 -0400'
  date_gmt: '2006-10-09 10:11:28 -0400'
  content: "I see your point. It is true that user experience is what finally draws
    the line between a successful and commonly used application and a well thought
    application that is simply not used.\r\n\r\nAs far as my experience with library
    catalog search applications is concerned I have always found the interface hard
    to use and difficult to understand (IBM370 terminals and such). Maybe because
    it was mainly directed towards making the most of the metadata accessible through
    search. But as an user I always had the problem to first understand how to map
    the concepts in my mind to the metadata used to map de concepts by the application
    and second to understand the search results returned by the application. Luckily
    the librarian was always near to lend a helping hand.\r\n\r\nTo be frank that&rsquo;s
    the main problem I see with metadata. It represents the world as the cataloguer
    understands it using his own knowledge base. But a different user may find it
    much harder to locate that very same book using concepts that are alien to him.
    So maybe a lot of metadata gets to a point where it is too much metadata and simply
    adds to the noise making it more difficult to retrieve the desired result from
    a search query.\r\n\r\nMaybe that&rsquo;s a strong point with Google. It has so
    many cataloguers available (web authors) that for every concept expressed by an
    user using words or word combinations it is able to locate several HTML pages
    where an author has expressed that very same concepts using similar words. It
    will provide also pages that are not closely related to the concepts the user
    had in mind. But attempting a best effort search over so many data (not metadata)
    is able to locate more results than the ones that would be possible to obtain
    through an exact term metadata search.\r\n\r\nSo maybe a good question is, when
    do a lot of metadata becomes too much metadata?"
- id: 5523
  author: the jester
  author_email: jester@DLTJ.org
  author_url: http://dltj.org/
  date: '2006-10-09 09:42:51 -0400'
  date_gmt: '2006-10-09 13:42:51 -0400'
  content: "Your perspective very interesting and somewhat refreshing -- thank you
    for continuing the conversation.\r\n\r\n[quote comment=\"5521\"]As far as my experience
    with library catalog search applications is concerned I have always found the
    interface hard to use and difficult to understand (IBM370 terminals and such).[/quote]\r\n\r\nBased
    on this comment and others in the context of this dialog that you are neither
    a librarian nor a library professional.  (That is what is making this conversation
    so refreshing!)  Please correct me if I'm wrong.\r\n\r\n[quote comment=\"5521\"]So
    maybe a good question is, when do a lot of metadata becomes too much metadata?[/quote]\r\n\r\nThis
    is a very keen observation, and I would offer the answer \"when the metadata gets
    in the way.\"\r\n\r\nIf I may speak for the library profession as a whole, there
    is a debate going on about the role of metadata in providing access to information.
    \ It has been argued that we spend too much time on the description of \"book\"
    and \"article\" items when simply a search across their text is all that is required
    to pull them up in response to a user's search request.  It has also been argued
    that now is definitely not the time to abandon rigorous description of content
    by cataloguers &mdash; that it is now more urgently needed with the explosion
    of information.\r\n\r\nMy own professional beliefs are somewhat scattered between
    these two extremes.  On the one hand, this rich metadata has already been created
    and paid for so we might as well use it to its greatest extent.  And \"use it\"
    does not mean the kind of in-your-face library catalog search applications that
    you rightly point out are hard to use and difficult to understand.  Rather that
    metadata can be used in more subtle ways to guide the user's discovery process
    as I hope is exemplified by the Amazon example.\r\n\r\nOn the other hand, I believe
    that we can no longer afford to pay for the human effort tied up in the description
    of textual materials (books and articles, mostly).  Amazon also shows us that
    it is possible to run computer algorithms across the corpus of textual material
    -- its Statistically Improbable Phrases and Capitalized Phrases -- that can approximate
    subject catalouging to the point where it is arguably \"good enough\" for the
    purposes of drawing together works on similar topics (which is what subject catalouging
    is all about anyway).  Instead the efforts of the library profession should be
    put towards the textual description of items that as yet defy an algorithmic approach:
    \ images, audio, and video, for instance.\r\n\r\nAnd on yet a third hand -- if
    I may have that many -- is the role of the user-as-catalouger through link analysis,
    social bookmarking, collective annotation, and lots of other useful \"web 2.0\"
    techniques.\r\n\r\nI'm still not sure Google is the best model for this, though,
    because it lacks one key ingredient:  selection.  Google's web crawlers attempt
    to look at everything in the web, index it, and make the retrieval results somehow
    usable to the end user.  In the next age of libraries &mdash; when perhaps we
    all have three hands &mdash; the selective application of computer algorithms
    coupled with user-driven annotation and professional description all over a targeted
    range of materials can \"out-Google\" the Google that we know today."
- id: 5526
  author: Sergio Berna
  author_email: sberna@gmail.com
  author_url: ''
  date: '2006-10-09 12:33:45 -0400'
  date_gmt: '2006-10-09 16:33:45 -0400'
  content: "First in answer to your question I&rsquo;m neither a librarian nor a library
    professional. I come from the other side of the problem, the technical one. My
    expertise has always been directed towards analysis, design and development of
    metadata driven applications such as document and record management systems.\r\n\r\nRight
    now for example, I&rsquo;m in the middle of a project involving fedora (Digital
    Object Repository) for the development of a records management system mainly oriented
    towards information preservation. Understanding by preservation that the content
    of the document must be accessible and usable in a far away future where technological
    innovation and evolution might render the previous format or system unusable.
    As such, we must not only provide the means so that the document format is updated
    accordingly to technical evolution but also so that the information provided to
    catalogue and locate the document (metadata) is evolved so that we also preserve
    the ability to locate that very same document.\r\n\r\nA good example would be
    converting all the pages contained in the library of congress to tiff and store
    them all away in a single HDD with a sequential name file contained in a single
    folder. We preserve the information as such, but nobody is going to be able to
    locate anything.\r\n\r\nAs such, in this kind of application, metadata collection,
    cataloguing and evolution is the key to a successful application.\r\n\r\nI located
    your page while performing a state of the art search and got a very pleasant surprise
    in finding that not only there is someone on the other side of the problem worrying
    about these things such as metadata and its technical implications, but also they
    know very well what they are talking about. No offense meant by the previous comment
    but it is rare to find non-technical people really worried about the technical
    problems related to their area of expertise as it is also very difficult to find
    technical people worrying about the real problem and not on how to solve it. Maybe
    that&rsquo;s why on most of the cases we come up with a very good solution for
    a problem that nobody has.\r\n\r\nReturning to the Google versus Amazon problem
    and your point of data contained in the document versus metadata about the document
    I would like to further follow your example search.\r\n\r\nIn order to do so I
    have written &ldquo;lion king cupcakes&rdquo; on the Google search interface and
    have located Amazon on the seventh position of the search result. Then I have
    changed the search to &ldquo;Lion King cupboards&rdquo; and have been amazed to
    find Amazon on the first and second position of the search. What&rsquo;s the difference?\r\n\r\nThe
    main difference is that the product located under the &ldquo;lion king cupboard&rdquo;
    search had 5 user reviews while the &ldquo;lion king cupcakes&rdquo; had none.
    Not all the people that reviewed the first product had the same point of view.
    In fact what makes Amazon the first option to be located is that it contains 5
    different points of view that depict the product as these 5 users see it. And
    two of them see the product fit for a cupboard.\r\n\r\nTo summarize it is not
    the metadata that Amazon knows about the product what drives me directly to it.
    It is the comment of a user in a Web 2.0 way that sees the product closely to
    the way I see it and that uses the very same words I have used in my search. It
    is also true that once I have located the product it is the metadata I know about
    the product what really makes it useful.\r\n\r\nMaybe that&rsquo;s the point behind
    the faceted metadata you mentioned earlier. The more facets your metadata has,
    the closest to a final user community it is and its usefulness in a search increases.\r\n\r\nAnother
    point is that maybe the process is divided in two parts. The first part is locating
    the object, the second part is making it usable. Metadata is the key to second
    part, but in searching and locating the object maybe too much metadata simply
    gets in the way."
- id: 5547
  author: the jester
  author_email: jester@DLTJ.org
  author_url: http://dltj.org/
  date: '2006-10-09 13:25:43 -0400'
  date_gmt: '2006-10-09 17:25:43 -0400'
  content: "If it helps explain my perspective, my university training is in systems
    analysis and I came into the library world purely by chance.  So I can really
    appreciate much of what you are describing.\r\n\r\nAlthough I couldn't reproduce
    your example using the North American Google search engine (based on your IP address
    I'm assuming you would be using the Spain edition of Google), I can agree with
    your assessment.  In the case of looking for &ldquo;lion king cupboard&rdquo;
    your Google search picked up the most relevant hits &mdash; I wonder if even Amazon's
    search engine would have picked them up in the comment fields of a product listing.\r\n\r\nStill,
    one has to wonder if at some point the raw keyword index across the entire web
    content is going to break down at some point.  (We could probably find some who
    will argue that it already has.)  In Amazon's database, those comments are part
    of the product listing's metadata (taking on a very liberal definition of the
    word \"metadata\" now).  In Google's database, it is most likely undifferentiated
    text.  As a discovery facet, for instance, I think it would be useful to the end
    user to know whether 'cupboard' appeared in the \"abstract\" of the item or in
    a comment about the item.  Well, not directly useful to the user, but the usefulness
    could be brought out in search results weighting, visual cues in the results listing,
    and post-search options (e.g. a checkbox to eliminate the occurrence of the word
    'cupboard' in all item comments).\r\n\r\nIn short, what I think we're agreeing
    on is that information retrieval in a \"web 2.0\" world is about three parts:
    \ the object itself, formal description or metadata, and annotations supplied
    by the end user.\r\n\r\nGlad to hear you are using FEDORA in your records management
    system.  That strengthens my believe that we are using the right system here at
    OhioLINK for content preservation and delivery."
- id: 5595
  author: Sergio Berna
  author_email: sberna@gmail.com
  author_url: ''
  date: '2006-10-10 06:08:03 -0400'
  date_gmt: '2006-10-10 10:08:03 -0400'
  content: "[quote post=\"94\"]Still, one has to wonder if at some point the raw keyword
    index across the entire web content is going to break down at some point.[/quote]\r\n\r\nMy
    personal opinion is that it will break at some point in the future. A good example
    of that is that our conversation now appears the fifth at Google while trying
    to locate pages using the &ldquo;lion king cupcakes&rdquo; term. And I would be
    very hard pressed to believe that for someone using that terms to locate a page,
    our conversation is useful in any way.\r\n\r\n[quote post=\"94\"]I think it would
    be useful to the end user to know whether &lsquo;cupboard&rsquo; appeared in the
    &ldquo;abstract&rdquo; of the item or in a comment about the item. Well, not directly
    useful to the user, but the usefulness could be brought out in search results
    weighting, visual cues in the results listing, and post-search options (e.g. a
    checkbox to eliminate the occurrence of the word &lsquo;cupboard&rsquo; in all
    item comments[/quote]\r\n\r\nYou have a good point there. Not all the words used
    in a textual object description have the same location weight. Google already
    knows that, but the only thing it is able to do is to assign weight depending
    on the page position, surrounding words and page references.\r\n\r\nThe real question
    behind is whether it can be done better without dramatically increasing the costs.
    And whether that cost increase has an adequate return in user perception.\r\n\r\nImagine
    that while writing our opinions we had written them all surrounded with appropriated
    metadata such as <abstract>&hellip;  <academicExample>Lion King Cupcakes</academicExample>
    &hellip; </abstract> or to use a better example lets imagine a book content indexing
    where we place metadata while indexing the content such as:\r\n\r\nAnd then <mainCharacter>Jhon</mainCharacter>,
    <secondaryCharacter>Mary </secondaryCharacter> traveled to <location>New York</location>.\r\n\r\nOn
    the first example in case Google had inferred that our intent is to buy something
    it could have stripped our conversation based on the fact that there those words
    where simply used as an example.\r\n\r\nOn the second example trying to locate
    a concrete book where the words Jhon and New York appear could be a real nightmare
    using a simple search and indexing engine. But maybe a book where Jhon is the
    main character is easier.\r\n\r\nAnd so we get to what I think is what you implied
    on your article. We have two totally different technologies. The first is text
    indexing. This technology ensures the best probability to locate a page related
    to the terms used while searching, provided that we have a rich and varied description
    of the subject. But it is the job of the user to  get something useful out of
    it.\r\n\r\nOn the other hand we have rich metadata cataloguing and truth inferencing
    engines where we can search in so many different ways that we can provide infinite
    customization to the query so that it reflects the exact user intent and locate
    only what the user wants / needs.\r\nOn the first case the job is performed by
    the user after the query, on the second case the job is performed before the query.\r\n\r\nWhich
    one is better?. Well the only thing I can say for sure is that the second is more
    expensive. My personal opinion is somewhere near yours in that a wise combination
    of both technologies might very well be the answer."
- id: 5644
  author: the jester
  author_email: jester@DLTJ.org
  author_url: http://dltj.org/
  date: '2006-10-11 09:45:53 -0400'
  date_gmt: '2006-10-11 13:45:53 -0400'
  content: "[quote comment=\"5595\"]My personal opinion is that [raw keyword indexing]
    will break at some point in the future. A good example of that is that our conversation
    now appears the fifth at Google while trying to locate pages using the &ldquo;lion
    king cupcakes&rdquo; term. And I would be very hard pressed to believe that for
    someone using that terms to locate a page, our conversation is useful in any way.[/quote]\r\n\r\nYes,
    I noticed that myself -- and was actually quite surprised how this one (dynamically
    generated) web page could leap up into the <a href=\"http://www.google.com/search?q=lion+king+cupcakes\"
    rel=\"nofollow\"> Google top 10 for 'lion king cupcakes'</a>.  At the very least
    I'm going to have to find a better example the next I demonstrate these concepts
    live!\r\n\r\n[quote]Not all the words used in a textual object description have
    the same location weight. Google already knows that, but the only thing it is
    able to do is to assign weight depending on the page position, surrounding words
    and page references.[/quote]\r\nRight!  Google can only make a guess based on
    context.  In some cases (feeds of vendor inventory for Froogle, perhaps raw marked-up
    versions of stories for Google News, etc.) Google may have access to the underlying
    structure, but their efforts to this point seem to be using those structural semantics
    to tweak the relevance ranking algorithm.  (Although there are some facets, such
    as 'price,' used in the Froogle interface.)\r\n\r\n[quote]We have two totally
    different technologies. The first is text indexing. This technology ensures the
    best probability to locate a page related to the terms used while searching, provided
    that we have a rich and varied description of the subject. But it is the job of
    the user to  get something useful out of it.\r\n\r\nOn the other hand we have
    rich metadata cataloguing and truth inferencing engines where we can search in
    so many different ways that we can provide infinite customization to the query
    so that it reflects the exact user intent and locate only what the user wants
    / needs.\r\n\r\nOn the first case the job is performed by the user after the query,
    on the second case the job is performed before the query.[/quote]\r\n\r\nAh, very
    clearly and succinctly stated.  That is the crux of the matter, I believe.  And
    I would agree that the second is somewhat expensive &mdash; particularly when
    it is human effort performing the metadata cataloging.  Where I see promise is
    in the decrease the cost of computing capacity and the improvement of algorithmic
    approaches to automated description.\r\n\r\nTo take a page from Clayton Christensen's
    theory of disruptive innovations:  is automated description of textual content
    good enough for some less-demanding users?  The answer I think is yes.  Is it
    good enough for high-demanding users as compared to human-driven description?
    \ No.  Will it ever be?  I think the answer here, too, is \"yes.\"  \r\n\r\n\"Good
    Enough\" &mdash; in this context &mdash; is the user's perceived performance of
    retrieval tools that use automated description versus those that use human-driven
    description.  If/when the perceived performance of the retrieval tool based on
    automated description is 'good enough,'  Christensen's model then goes on to say
    that user choice is based on other factors such as 'cost'.  Assuming for the moment
    that the information technology solution will be cheaper than the human-driven
    solution, the users will use the information technology solution.\r\n\r\nOnly
    time will tell, of course, how this will all pan out..."
- id: 5727
  author: Sergio Berna
  author_email: sberna@gmail.com
  author_url: ''
  date: '2006-10-17 08:30:01 -0400'
  date_gmt: '2006-10-17 12:30:01 -0400'
  content: "[quote post=\"94\"]To take a page from Clayton Christensen&rsquo;s theory
    of disruptive innovations: is automated description of textual content good enough
    for some less-demanding users? The answer I think is yes. Is it good enough for
    high-demanding users as compared to human-driven description? No. Will it ever
    be? I think the answer here, too, is &ldquo;yes.&rdquo; [/quote]\r\n\r\nVery good
    point. The only thing I would add is that my impression as a technologists is
    that \"will it ever be?, yes\", and very soon.\r\n\r\nWe have the tools, we have
    the knowledge and we have the chance.\r\n\r\nStarting with projects such as <a
    href=\"http://wordnet.princeton.edu/\" rel=\"nofollow\">Wordnet</a>, or <a href=\"http://www.illc.uva.nl/EuroWordNet/\"
    rel=\"nofollow\">Eurowordnet</a>.\r\nText indexing initiatives such as <a href=\"http://www.google.com\"
    rel=\"nofollow\">google</a>. Metadata cataloguing and triple stores search engines.
    And a lot of other technologies.\r\n\r\nAnd the most important tool of all, Web
    2.0 interfaces that help to bring the user into the application my impression
    is that we are in the verge of a huge change.\r\n\r\nMy impression is that up
    to now internet has been considered only as a huge automatic vending machine where
    I can simply place an order and get a result. It is curious but with all the technology
    available I can hardly see any portal where all the things they do cant be traced
    back to the automatic vending machine example.\r\n\r\nLets examine two distinct
    and extreme situations that I think reflect the extent of the problem.\r\n\r\nWould
    you ever consider closing the library and placing instead an automatic book dispenser
    machine such as most of the video stores do?. I think not.\r\n\r\nWould you, in
    the other hand, close the retail shop and place all your employees in a portal
    where they can replicate the user retail shop experience through web technologies?.
    To the best of my knowledge &ldquo;It has not been done&rdquo;.\r\n\r\nLet me
    further follow the last example so that it might be fully understood. In this
    way imagine all your employees working at a contact center and available to answer
    Internet Telephony, Video Conferences, Online Chats, email requests and any other
    way of internal communication and cooperative navigation. In this way, when a
    &ldquo;would be customer&rdquo; enters into the portal (retail shop) he might
    look around a bit (search and navigation) but up into the portal he suddenly can
    read a message such as &ldquo;welcome, can I help you?&rdquo;, or he might see
    that there are 4 people ready to help him and upon selection a chat window, or
    audio / video conference is started as in a normal &ldquo;real life&rdquo; buying
    experience.\r\n\r\nIn the middle of those two extreme initiatives comes this kind
    of technology we are talking about. What we are always trying to do is to emulate
    the user face to face real life buying experience through technology.\r\n\r\nIn
    real life the customer comes into the shop. The librarian is able to see how the
    user is dressed, what books is he looking at. He is able to speak with him and
    in case he is a very good sales man to drive the user towards the thing he needs,
    using non-structured language. Finally what drives the difference between a good
    seller and a bad one is that he can know faster what the user wants through both
    visual and conversational skills. To summarize, he is able to &ldquo;bring the
    customer into the deal&rdquo;.\r\n\r\nI think this notion of &ldquo;bringing the
    customer in&rdquo; is what drives web 2.0 initiatives as such. And is the real
    breakthrough in my opinion."
- id: 5783
  author: the jester
  author_email: jester@DLTJ.org
  author_url: http://dltj.org/
  date: '2006-10-19 10:48:49 -0400'
  date_gmt: '2006-10-19 14:48:49 -0400'
  content: "[quote comment=\"5727\"]My impression is that up to now internet has been
    considered only as a huge automatic vending machine where I can simply place an
    order and get a result. It is curious but with all the technology available I
    can hardly see any portal where all the things they do cant be traced back to
    the automatic vending machine example.[/quote]\r\n\r\nAs I think about it, I'm
    finding this to be a very useful analogy (not only for library sites but service
    websites at large).  Of course we wouldn't replace the library with an automated
    vending machine, yet our websites push the 'customer' that way.  Nor do we want
    to.\r\n\r\nAs you point out, there is a fine line between \"stalking\" the user
    (either through the website or in the physical world) and being readily available
    for help.  Being 'readily available' also is an expense in human capital that
    needs to be used wisely, so as much as possible the technology needs to bring
    make that human-to-human contact as effective as possible.  As you point out,
    perhaps one of the value-add for libraries to out-Google Google is that human
    touch to know \"faster what the user wants through both visual and conversational
    skills.\""
- id: 162808
  author: Ken Cooper
  author_email: ''
  author_url: http://twitter.com/kencooperusa/status/4852078010699776
  date: '2010-11-17 11:03:20 -0500'
  date_gmt: '2010-11-17 16:03:20 -0500'
  content: <span class="topsy_trackback_comment"><span class="topsy_twitter_username"><span
    class="topsy_trackback_content">Can Google be Out-Googled? http://bit.ly/bnnVEx</span></span>
- id: 678759
  author: Google - BC$ MobileTV Wiki
  author_email: ''
  author_url: http://wiki.bcmoney-mobiletv.com/index.php?title=Google
  date: '2015-04-04 14:28:25 -0400'
  date_gmt: '2015-04-04 18:28:25 -0400'
  content: "<!--%kramer-ref-pre%-->[&#8230;] Can Google be Out-Googled?: http://dltj.org/article/can-google-be-out-googled/
    [&#8230;]<!--%kramer-ref-post%-->"
---
<p>I have been heard to remark to other librarians on occasion a comment along the lines of  "Don't fear Google; Don't Chase Google; Let's Out-Google Google!"  After allowing the confused stare linger for a moment or the hysterical laughter die down, I explain my thesis:  we have something Google doesn't have &mdash; no, it isn't the selective care with which we select "authoritative" material (the <a href="http://en.wikipedia.org/wiki/PageRank" title="http://en.wikipedia.org/wiki/PageRank">PageRank</a> algorithm does a pretty good job at that); and no, it isn't our <a href="http://library.csun.edu/About/ASRS" title="Automated Storage &amp; Retrieval System (ASRS)">warehouses of books</a> (the <a href="http://books.google.com/googleprint/library.html" title="Google Book Search Library Project">Google Book Search</a> project will pretty effectively capture that) &mdash; we have faceted metadata.  And lots of it.</p>
<p>Google is a big, full-text search engine.  It has various algorithms for parsing the content of a web page to determine what pieces are more important than others, other algorithms for examining user's natural language query to guess what the user is really seeking (sound like a reference interview?) then yet more algorithms for relevance ranking that will push certain pages to the forefront.  And guess what, it (and its peer search engines) do a pretty good job.  But at their core they are still full-text search engines and a lot of educated guessing.</p>
<p>Libraries, on the other hand, are awash with metadata.  And, based on <a href="http://boxesandarrows.com/ranganathan-for-ias/" title="Ranganathan for IAs - Boxes and Arrows: The design behind the design">our obsession with it</a>, we do a pretty good job.  At OhioLINK, I've been privy to many a meeting where the merits of one vendor's metadata structure and quality were pitted against another, always in pursuit of the best possible metadata surrogate for our users.</p>
<h2>Who is Our Real Competition?  Google or Amazon?</h2>
<p>To bring my out-Google Google thesis into sharp relief, I offer:  with respect to search engines, who is a library's biggest competitor &mdash; Google or Amazon?  Okay, let me ask it another way.  Say you have a four-year-old who adores Disney's The Lion King (not a big stretch for my imagination -- how about you?) and she wants to have all sorts of related artifacts for her birthday party.  You'd like to bake a cake or cupcakes or something like that for her.  Where would you start your search?  Google or Amazon?</p>
<p>If you started from <a href="http://www.google.com/search?q=lion+king+cupcakes" title="lion king cupcakes - Google Search">Google with a search for "Lion King Cupcakes"</a>:</p>
<div style="padding: 10px; margin: 0.67em auto; border: thin solid silver;">
<img id="image95" src="/wp-content/uploads/2006/07/Google-lion_king_cupcakes.png" alt="Google Search for "Lion King Cupcakes"" /></p>
<p style="font-family: verdana, arial, sans-serif; font-style: italic; font-size: smaller; padding-left: 1%; padding-right: 1%; line-height: 1.1; margin: 0.25em auto 0 auto;">
Google Search Results for "Lion King Cupcakes".  <em>Note: to save space in the screen-shot, the Google-supplied advertisements and irrelevant portions of the page have been removed.</em></p>
</div>
<p>Ironically (for this example) your first two hits are into Amazon pages.  The next two hits are to a party store -- also a likely source of useful stuff, but you'd actually have to visit those pages to be sure.  (Google's algoriths are just guessing what is on the party store pages, after all, right?)  The last three hits are likely not relevant at all...a Blogspot page "posted by Cupcake", an entry into the IMDB for one of the voice actors ("cupcake" shows up in the title of another one of the actor's films), and an-odd-and-likely-irrelevant page from MySpace with the title "www.myspace.com/killsxthrills".  After looking through these pages, you'd then have to do another search for "Lion King Cake" and wade through those search results as well.</p>
<p>If you started from <a href="http://www.amazon.com/s/ref=nb_ss_gw/url=search-alias%3Daps&#038;field-keywords=lion+king&#038;Go.x=0&#038;Go.y=0&#038;Go=Go" title="">Amazon with a search for "Lion King"</a> (more general than the Google search, and you'll see why that is better in a moment) you'd get the expected results of soundtracks, movies, etc. in the main search result area:</p>
<div style="padding: 10px; margin: 0.67em auto; border: thin solid silver;">
<img id="image96" src="/wp-content/uploads/2006/07/Amazon-lion_king_1.png" alt="Amazon Search for "Lion King"" /></p>
<p style="font-family: verdana, arial, sans-serif; font-style: italic; font-size: smaller; padding-left: 1%; padding-right: 1%; line-height: 1.1; margin: 0.25em auto 0 auto;">
Amazon search for "Lion King"</p>
</div>
<p>But along the left side is a breakdown by, in Amazon's case, product type.  Look at the green-highlighted one -- "Gourmet Food" ... that sounds promising:</p>
<div style="padding: 10px; margin: 0.67em auto; border: thin solid silver;">
<img id="image97" src="/wp-content/uploads/2006/07/Amazon-lion_king_2.png" alt="Amazon Search for "Lion King" Refined" /></p>
<p style="font-family: verdana, arial, sans-serif; font-style: italic; font-size: smaller; padding-left: 1%; padding-right: 1%; line-height: 1.1; margin: 0.25em auto 0 auto;">
Amazon search results for "Lion King", refined to "Gourmet Food".</p>
</div>
<p>Ah!  Here we go:  one that I know I can throw out because I'm not interested in fruit snacks and three &mdash; two decorating kits and one pre-made party favor cookies (no baking on my part!) &mdash; that are clearly relevant.  And just in case the were to be a flood of foodstuffs related to The Lion King, the faceted breakdown continues on the left side with the "Narrow by Category", "Narrow by Cuisine", "Narrow by Brand", and "Narrow by Price" options.</p>
<p>That's the power of metadata in a search context.  As libraries, we pay a lot of money for that metadata but don't make nearly as much use of it as we could in our search and browse interfaces.</p>
<h2>Is Out-Googling Possible?  A Brief History of the Networked Information World</h2>
<p>With me so far?  So faceted browse/search with our existing rich metadata is s good thing.  But can we really take on the Google Giant?  Even the absence of insider information or a business plan to take on Google, you have have to consider an emphatic "yes!" as the answer.  Here's why.</p>
<p>Remember Gopher?  That was going to revolutionize the world of information retrieval, and "<a href="http://www.ou.edu/research/electron/internet/veronica.htm" title="Veronica FAQ (Part 1 of 2)">Veronica</a>" (the <strong>v</strong>ery <strong>e</strong>asy <strong>r</strong>odent-<strong>o</strong>riented <strong>n</strong>et-wide <strong>i</strong>ndex of <strong>c</strong>omputerized <strong>a</strong>rchives) was right there helping us ferret out (sorry for the third rodent reference...) information from the thousands of Gopher servers around the world. <footnote>In case the University of Oklahoma libraries decides that Veronica information on their website is no longer relevant, you can pull up <a href="http://web.archive.org/web/*/http://www.ou.edu/research/electron/internet/veronica.htm" title="Internet Archive Wayback Machine">this page out of the Internet Archive</a>.</footnote></p>
<p>But then this web thing came a long, and we dropped Gopher/Veronica (and WAIS, for that matter) quicker than a chipmunk being chased by a fire-breathing dragon (okay, that really stretched the rodent-to-firefox metaphor).  Instead we picked up Mosaic and its follow-ons, and a new search engine &mdash; <a href="http://web.archive.org/web/19960511013133/http://www.altavista.digital.com/" title="AltaVista: Main Page">AltaVista</a>.  How many of us (that were around at the time) set our browser home pages to AltaVista?  Nothing could beat it, and we all thought we were seeing the end of libraries.</p>
<p>Then what ... remember?  <a href="http://web.archive.org/web/19961017235908/http://www2.yahoo.com/" title="Yahoo!">Yahoo came onto the scene</a> and we all set our home pages to <em>that</em>.  Yahoo was indexing the internet our way (with labels and hierarchies), and was searchable too.  Did we think that anything could stop Yahoo?</p>
<p>Has your home page (or at least your information discovery tool, a.k.a. "search engine") changed since the AltaVista and Yahoo days?  Did you have a passing fancy with <a href="http://web.archive.org/web/19971210205553/http://www.northernlight.com/" title="Northern Light Search">NorthernLight</a> (blue folders!)?  Have you eyed other tools?</p>
<p>If you answered "yes" to any of those questions, can you really say that Google will remain supreme?  And if not Google, then what?  Could we (the library community <i>writ large</i>) build a knowledge discovery using as a source all of the faceted metadata we've produced in the last 30 years (MARC catalogs, Indexing/Abstracting, Citations, etc.)?  And if we did it more like Amazon that Google, could we out-Google Google?</p>
<h2>Doubts To My Own Vision</h2>
<p>Can we do it?  As the lead (and perhaps only) cheerleader for the Out-Google Google thesis, I'm starting to have my doubts.  First came <a href="/article/automated-faceted-analysis-in-google/">what looked like automated faceted analysis in Google</a>.  That turned out to be <span class="removed_link" title="http://www.google.com/coop/topic?cx=health_devel">Google Co-op Health</span> &mdash; a human-driven effort to add metadata to selected websites that appear in the search engine results.  (By the way...why didn't we [the library profession] think of that?  And now that it has been thought of why aren't we doing more with enlisting the aid of experts from their own field in categorizing their segment of the world of information?)</p>
<p>The second, through an odd time warp, was an article in the Washington Post from last November called <a href="http://www.washingtonpost.com/wp-dyn/content/article/2005/11/11/AR2005111101644.html" title="What Lurks in Its Soul?">What Lurks in Its Soul?</a>  (I have to say "odd time warp" because although the article was published nine months ago, I only ran across it today after a <span class="removed_link" title="http://www.technorati.com/search/%26quot%3Bdisruptive+change%26quot%3B+AND+%28libraries+OR+library%29">Technorati search for "disruptive change AND (libraries OR library)"</span> pulled up a blog entry from four months ago on the article.)  Here is the lead paragraph:</p>
<blockquote>
<h2>What Lurks in Its Soul?</h2>
<p>By David A. Vise</p>
<p>Sunday, November 13, 2005; Page B01</p>
<p>The soul of the Google machine is a passion for disruptive innovation.</p>
<p>Powered by brilliant engineers, mathematicians and technological visionaries, Google ferociously pushes the limits of everything it undertakes. The company's DNA emanates from its youthful founders, Sergey Brin and Larry Page, who operate with "a healthy disregard for the impossible," as Page likes to say. Their goal: to organize all of the world's information and make it universally accessible, whatever the consequences.
</p></blockquote>
<p>So I'm not sure anymore &mdash; my faith in our ability to win a black-and-white, us-versus-them battle with Google for knowledge retrieval supremacy has been shaken.  Do we cede that ground to Google?  Can Google, in an Innovator's Dilemma disruptive fashion, be out-Googled?  Are we, the library community, the one's to do it?</p>
<p>Your thoughts?</p>
<p>[Edited 20060731T0826 to fix HTML and text typos.]</p>
<p style="padding:0;margin:0;font-style:italic;" class="removed_link">The text was modified to remove a link to http://www.google.com/coop/topic?cx=health_devel on November 17th, 2010.</p>
<p style="padding:0;margin:0;font-style:italic;">The text was modified to update a link from http://library.csun.edu/About_the_Library/asrs.html to http://library.csun.edu/About/ASRS on December 31st, 2010.</p>
<p style="padding:0;margin:0;font-style:italic;">The text was modified to update a link from http://www.boxesandarrows.com/view/ranganathan_for_ias to http://boxesandarrows.com/view/ranganathan_for_ias on August 22nd, 2013.</p>
