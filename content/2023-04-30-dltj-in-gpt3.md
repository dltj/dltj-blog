---
title: This website contains 0.00006% of the world's knowledge
modified: '2023-04-30T20:50:13-04:00'
category: Raw Technology
categories:
- Raw Technology
tags:
- large language model
mastodon:
- New blog post
---
According to reputable sources, this blog contains 0.00006% of the world's knowledge.

1. The large language models (LLMs) that underlie tools like ChatGPT and Bing-AI are being used as question-answering tools. If you listen to the hype surrounding what LLMs can do, you can hardly be faulted for thinking that is has every fact known to humankind and can answer any question.
1. One of the most popular large language models, GPT-3, was trained with several large text datasets. 
1. One dataset, C4 (a filtered version of the Common Crawl), is 60% of the text used in training. 
1. According to {{ robustlink(href="https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/", versionurl="https://web.archive.org/20230420094022/https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/", versiondate="2023-04-19T21:08:15", title="Inside the secret list of websites that make AI like ChatGPT sound smart | Washington Post", anchor="this article in the Washington Post") }}, dltj.org is 0.0001% of the tokens in the C4 dataset.

{{ image(width="", localsrc="2023/2023-04-29-dltj-in-washingtonpost-ai-chatbot-learning.svg", caption="Screen capture of 'dltj.org' search results in Washington Post article.") }}

How much is 0.0001% of the GPT-3 training set? 
It is a quarter of an inch (half a centimeter) off sea level on a climb up Mount Everest. (Source: {{ robustlink(href="https://www.wolframalpha.com/input?i=How+much+is+0.00006%25+of+the+height+of+Mount+Everest%3F", versionurl="https://media.dltj.org/", versiondate="2023-04-29", title="'https://www.wolframalpha.com/input?i=How+much+is+0.00006%25+of+the+height+of+Mount+Everest%3F' | Wolfram|Alpha", anchor="Wolfram Alpha") }}) 
It is almost 8 feet (2.5 meters) of a journey from Washington, DC, to San Francisco, California (Source: {{ robustlink(href="https://www.wolframalpha.com/input?i=how+much+is+0.00006%25+of+the+distance+between+Washington+DC+and+San+Francisco%3F", versiondate="2023-04-29", title="'How much is 0.00006% of the distance between Washington DC and San Francisco?' | Wolfram|Alpha", anchor="Wolfram Alpha") }}) 
In contrast, the content from the New York Times is 0.036% of the training dataset, or 9/10ths of a mile (1.4km) on that journey.

(A note about assumptions: OpenAI hasn't published the contents of the training data for GPT-3.5—which is used in ChatGPT—and GPT-4. So this post uses the data from GPT-3 {{ robustlink(href="https://en.wikipedia.org/wiki/GPT-3#Training_and_capabilities", versionurl="https://en.wikipedia.org/w/index.php?title=GPT-3&oldid=1152388451#Training_and_capabilities", versiondate="2023-04-29", title="GPT-3 | Wikipedia", anchor="as listed in Wikpedia") }}. )

You can use the search tool near the bottom of the {{ robustlink(href="https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/", versionurl="https://web.archive.org/20230420094022/https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/", versiondate="2023-04-19T21:08:15", title="Inside the secret list of websites that make AI like ChatGPT sound smart | Washington Post", anchor="Washington Post article") }} to see where your favorite website ranks. 
But also read the article to explore what is in the <a href="https://paperswithcode.com/dataset/c4">C4 version</a> of the <a href="https://commoncrawl.org">Common Crawl</a>. 
As much as OpenAI is trying to put guardrails on the output, the model itself is trained on some pretty offensive stuff. 

