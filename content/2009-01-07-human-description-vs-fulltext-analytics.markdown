---
layout: wordpress-import
status: publish
published: true
title: 'For the heart and soul of librarianship -- human description versus fulltext analytics'
modified: 2009-01-07T21:57:27+00:00
author: Peter Murray
author_login: lyrdor
author_email: jester@dltj.org
author_url: http://dltj.org/about
wordpress_id: 652
wordpress_url: http://dltj.org/?p=652
date: '2009-01-07 16:57:27 -0500'
date_gmt: '2009-01-07 21:57:27 -0500'
categories:
- L/IS Profession
tags:
- description
- Google Book Search
comments:
- id: 34502
  author: Ron Murray
  author_email: rmur@loc.gov
  author_url: ''
  date: '2009-01-08 14:17:03 -0500'
  date_gmt: '2009-01-08 19:17:03 -0500'
  content: "The descriptionist/fulltext analytics (perhaps better thought of as computationalist
    - that way we can deal with certain aspects of image and sound description) question
    is indeed an interesting one to pose.\r\n\r\nI think the computationalists can
    do a decent job of relating their assumptions to a broad range of theories in
    computer and cognitive sciences, statistics, and linguistics.\r\n\r\nI should
    like to see the descriptionists do likewise in attaching their assertions to fields
    of inquiry beyond that of the library (but amply represented within the library).
    \r\n\r\nDescriptionists might try avoiding the easy reach for its philosophical
    roots and pay a great deal more attention to psychology, linguistics, anthropology,
    and sociology.\r\n\r\nEach of these fields have much to say about the kinds of
    behavior indulged in when one describes a Resource for oneself or on behalf of
    others."
- id: 34508
  author: the Jester
  author_email: jester@dltj.org
  author_url: http://dltj.org/about
  date: '2009-01-09 11:24:25 -0500'
  date_gmt: '2009-01-09 16:24:25 -0500'
  content: "Ron --\r\n\r\nI like the term computationalist because, as you suggest,
    computer analysis can be performed on more than just text.  The terms 'descriptionist'
    and 'fulltext analytics' went through several iterations in drafts of this post,
    and I was never quite comfortable with them.  Perhaps my remaining doubts are
    that the terms relate to the actors doing the description.  The term 'descriptionist'
    (in my mind, at least) implies the human element.  A 'computationalist' is a human
    actor as well, but one that creates the algorithms.  It is the execution of the
    algorithms themselves that generates the description -- akin to the activity that
    the descriptionist performs.\r\n\r\nThat the computationalist and the descriptionist
    bring different behaviors to the act of creating a surrogate for the target item
    is why I think a combination of the two is important."
- id: 34534
  author: the Jester
  author_email: jester@dltj.org
  author_url: http://dltj.org/about
  date: '2009-01-16 12:30:37 -0500'
  date_gmt: '2009-01-16 17:30:37 -0500'
  content: A colleague sent a private note about the term computationalist and the
    implied relation to the field of <a href="http://en.wikipedia.org/wiki/Computational_linguistics"
    rel="nofollow">'computational linguistics'</a>.  That is an interesting observation
    because it starts to get at the meaning behind the strings of words that Google
    is digitizing -- or the "concepts" as I say in the original post.  I wish I had
    more time to study the intersection of these two areas -- the descriptionist and
    the computationalist.  This seems like a rich area of exploration.
---
<p>A non-librarian colleague forwarded a link to an essay by <a href="http://blog.futurestreetconsulting.com/?page_id=8" title="About Mark Pesce">Mark Pesce</a> called <a href="http://blog.futurestreetconsulting.com/?p=101" title="The Alexandrine Dilemma | the human network">The Alexandrine Dilemma</a>.  From the context of <a href="http://blog.futurestreetconsulting.com/?p=101#comment-25985" title="Comment on &#039;The Alexandrine Dilemma&#039;">one of the comments</a>, I think it might have been the text of a keynote given at <a href="http://conferences.alia.org.au/newlibrarian2008/" title="New Librarians Symposium home page">New Librarians Symposium</a> in Australia last month.  It is a thought-provoking piece that, well, provoked some thoughts.</p>
<h2>Some Corrections</h2>
<p>As I was reading through it, my train of thought was tripped up by a couple of occasions in the text that needed correction.  A comment describing these have been pending in the blog author's moderator queue for a couple of days, so I'm adding it here in case it gets lost there.  These are minor points, and don't detract too much from Pesce's essay.  First, Wikipedia's current content license is the <a href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Text of the GNU Free Documentation License">GNU Free Documentation License</a> for one, not a Creative Commons license (the full explanation of the <a href="http://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Copyrights of Wikimedia content">Wikipedia Copyrights</a> is available).  The GFDL is not the same as the Creative Commons licenses.  The <a href="http://arstechnica.com/news.ars/post/20081104-amended-fdl-will-allow-wikipedia-to-adopt-cc-license.html" title="Amended FDL will allow Wikipedia to adopt CC license">recently amended GFDL will allow Wikipedia to adopt</a> the CC-BY-SA license (according to the <a href="http://www.gnu.org/licenses/fdl-1.3-faq.html" title="FDL 1.3 FAQ - GNU Project - Free Software Foundation">GFDL FAQ</a>.  Wikimedia has a <a href="http://commons.wikimedia.org/wiki/Commons:Reusing_content_outside_Wikimedia#Text" title="Reusing content outside Wikimedia - Wikimedia Commons">very complicated chart</a> on what licenses can be used when reusing Wikimedia content, but seems to be migrating towards Creative Commons licenses in general.</p>
<p>Second, Google is not scanning the whole Harvard library; it is <a href="http://hul.harvard.edu/hgproject/faq.html" title="Harvard University Library : Harvard-Google Project : FAQ">only scanning the public domain works</a> at the library.  After reviewing the terms of the settlement agreement with the book authors and book publishers, Harvard reaffirmed its <a href="http://www.thecrimson.com/article.aspx?ref=524989" title="The Harvard Crimson :: News :: Harvard-Google Online Book Deal at Risk">decision to limit its participation</a> to public domain books only.  I think it also important to note that the presentation of in-copyright books as you described should be in the future tense; the settlement agreement has not been finalized and money from subscribers/purchases of books under the Google book reader interface is not yet being collected.</p>
<h2>The Heart of the Matter</h2>
<p>My colleague chose to pull out these three paragraphs as hooks to get me and others to read the article.</p>
<blockquote><p>&hellip;the basic point is this: wherever data is being created, that&rsquo;s the opportunity for library science in the 21st century. Since data is being created almost absolutely everywhere, the opportunities for library science are similarly broad. It&rsquo;s up to you to show us how it&rsquo;s done, lest we drown in our own creations.</p>
<p>The dilemma that confronts us is that for the next several years, people will be questioning the value of libraries; if books are available everywhere, why pay the upkeep on a building? Yet the value of a library is not the books inside, but the expertise in managing data. That can happen inside of a library; it has to happen somewhere. Libraries could well evolve into the resource the public uses to help manage their digital existence. Librarians will become partners in information management, indispensable and highly valued.</p>
<p>In a time of such radical and rapid change, it&rsquo;s difficult to know exactly where things are headed. We know that books are headed online, and that libraries will follow. But we still don&rsquo;t know the fate of librarians. I believe that the transition to a digital civilization will flounder without a lot of fundamental input from librarians. We are each becoming archivists of our lives, but few of us have training in how to manage an archive. You are the ones who have that knowledge. Consider: the more something is shared, the more valuable it becomes. The more you share your knowledge, the more invaluable you become. That&rsquo;s the future that waits for you.</p></blockquote>
<p>I don't question Pesce's premise that the more something is shared, the more valuable it becomes.  In particular, I believe that is the basis of many of the <a href="http://wiki.code4lib.org/index.php/OCLC_Policy_Change" title="OCLC Policy Change">arguments being made against OCLC's new Records Use Policy</a>.  The question in my mind, though, is the notion that librarians are the privileged holders of keys to the information management lock-box.  I question his premise that library science has <em>the</em> way to manage information; that is, through careful examination, description and categorization of information.  Library science, with these techniques, certainly has <em>a</em> way to deal with information overload, but certainly not the only way.</p>
<p>One of Pesce's examples is the <a href="http://books.google.com/" title="http://books.google.com/">Google Book Search</a> project.  With it, Google is testing a very powerful paradigm -- that is that it is easer to search than it is to sort.  Or, more accurately, it is becoming easier for computer algorithms to ferret out the information being sought than it is for library science practitioners to categorize it in a standard vocabulary.  Pesce's calls the reliance on computer algorithms "a beginner&rsquo;s mistake" among "Google&rsquo;s army of PhDs".  I'd call it a fascinating experiment.</p>
<h2>Google's Experiment</h2>
<p>The axioms of Precision and Recall are well known in the library science field.  Recall is a measure of effectiveness in retrieving (or selecting) performance and can be viewed as a measure of effectiveness in including relevant items in the retrieved set.  Precision is a measure of purity in retrieval performance, a measure of effectiveness in excluding nonrelevant items from the retrieved set. ((Michael Buckland and Fredric Gey. "The Relationship between Recall and Precision" Journal of the American Society of Information Science. 45(1): 12-19.))  As a general rule, Precision and Recall are inversely related:  if you construct a search geared towards high Precision, you loose Recall -- that is you'll miss some of everything that is out there.  The opposite is also true:  a high Recall search results in low Precision -- lots of extraneous stuff that you don't want.  The key characteristic of any information retrieval system is to push the boundaries of Precision versus Recall as far as they will go.</p>
<p>Traditional library science invokes to tools of descriptive surrogates for the item itself.  Specifically, the use of controlled vocabularies.  We have taxonomies of names (the most popular in North America is the <a href="http://authorities.loc.gov/" title="Library of Congress Authorities (Search for Name, Subject, Title and Name/Title)">Library of Congress Name Authority File</a>).  We also have rich ontologies for subjects and topics, be they general like the <a href="http://en.wikipedia.org/wiki/Library_of_Congress_Subject_Headings" title="Tools for Authority Control--Subject Headings">Library of Congress Subject Headings</a> or specialized like the <a href="http://www.nlm.nih.gov/mesh/" title="Medical Subject Headings homepage">Medical Subject Headings</a>.  In all cases, a skilled 'descriptionist' (a.k.a. a cataloging librarian) distills the nature of the item into a descriptive record at a level of characterization that the descriptionist will strike a balance for users between Precision and Recall.</p>
<p>Google, on the other hand, relies on an analysis of the text of the item itself to be the descriptive surrogate.  It employs algorithms that look at every word -- perhaps even every concept ((See <a href="/article/the-big-switch/">&ldquo;We are scanning them to be read by an AI.&rdquo;</a> for a discussion around Google's desire to scan books to be read by an artificial intelligence engine.)) -- in an item and weights it relative to those words and terms in other items in the corpus.  I doubt that this technique is new, but the scale to which Google is applying this technique -- to all knowledge recorded in all available books in major world libraries -- is definitely new.</p>
<h2>Descriptionists versus fulltext analytics</h2>
<p>So which is better: the descriptionist technique or the fulltext-analytics technique?  For the previous hundreds of years, the descriptionist technique was undoubtedly the best, and this seems to be the thesis of Pesce's article.  But the world is changing.  Has the recent availability of cheap computing power and the invention of new algorithms changed the balance towards the fulltext-analytics technique?  Google seems to think so.  At the very least, they have plunked down a sizable chunk of cash to test out that proposition.</p>
<p>The best answer is probably some combination of the description and automated analysis, and that is a puzzle no computer will be able to solve.  I think the next challenge for the field of library science is to explore the balance between descriptionists and fulltext-analytics to employ the most rational and cost-effective uses of both.  Understanding these techniques and balancing them is dilemma that confronts us.
<p style="padding:0;margin:0;font-style:italic;">The text was modified to update a link from http://www.loc.gov/cds/lcsh.html to http://en.wikipedia.org/wiki/Library_of_Congress_Subject_Headings on November 17th, 2010.</p>
